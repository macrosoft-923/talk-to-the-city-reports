<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="preload" href="./_next/static/css/d1386df84e9d164b.css" as="style"/><link rel="stylesheet" href="./_next/static/css/d1386df84e9d164b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="./_next/static/chunks/webpack-8e5e5c873be2759d.js" defer=""></script><script src="./_next/static/chunks/framework-052b50cd3d4947f2.js" defer=""></script><script src="./_next/static/chunks/main-98b8f49aadcd70c5.js" defer=""></script><script src="./_next/static/chunks/pages/_app-86112568b43b1067.js" defer=""></script><script src="./_next/static/chunks/3e199aef-89d7c524effe644b.js" defer=""></script><script src="./_next/static/chunks/49-3d197e88747a15b8.js" defer=""></script><script src="./_next/static/chunks/106-a71b11894cb18610.js" defer=""></script><script src="./_next/static/chunks/621-67f971e1eb395bea.js" defer=""></script><script src="./_next/static/chunks/pages/index-dca75ab717967b15.js" defer=""></script><script src="./_next/static/9jyuozYdQoy50oWM2XFRY/_buildManifest.js" defer=""></script><script src="./_next/static/9jyuozYdQoy50oWM2XFRY/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"result":{"clusters":[{"cluster":"高齢者向けの理解の難しさ","cluster_id":"2","takeaways":"高齢者向けの情報提供には、簡潔でわかりやすい表現が求められています。多くの参加者が、初心者には難しい内容や特定の機種に関する不明点を指摘しました。また、実際に操作しながら学ぶことが効果的であり、間違えた際のフォローが重要だと強調されています。\n\n一方で、70歳代でも理解できる内容を求める声や、キャッシュレス決済の多様性についての知識不足を感じる意見もありました。著者の実践的なアプローチが評価される一方で、万人に合う内容を提供する難しさも浮き彫りになっています。","arguments":[{"arg_id":"A1_0","argument":"高齢者には難しい語句が多く含まれている。","comment_id":"1","x":6.8983545,"y":8.727041,"p":1},{"arg_id":"A5_0","argument":"通販では、必要な情報が見つからないことがある。","comment_id":"5","x":7.2100425,"y":10.60301,"p":0.8405943239099011},{"arg_id":"A6_1","argument":"高齢者にとって非常に適していると感じる。","comment_id":"6","x":6.851038,"y":8.654147,"p":1},{"arg_id":"A8_0","argument":"私は知りたいことが理解できない場合が多い。","comment_id":"8","x":7.086517,"y":10.166238,"p":0},{"arg_id":"A9_0","argument":"使い方がわからない時にこの本を使うことがある。","comment_id":"9","x":7.37387,"y":11.034832,"p":0.91594491264353},{"arg_id":"A14_0","argument":"私はもっと驚くような使い方を期待していたが、あまり参考にならなかった。","comment_id":"14","x":7.4562573,"y":9.626232,"p":1},{"arg_id":"A15_0","argument":"この本は、メールやWebの基本ができる人向けに書かれている。","comment_id":"15","x":7.331888,"y":11.438185,"p":0},{"arg_id":"A15_1","argument":"まったくの初心者には内容が難しく感じられる。","comment_id":"15","x":6.293159,"y":10.848359,"p":1},{"arg_id":"A17_1","argument":"お年寄りは、実際に触りながら間違えながら覚えることで、より効果的に学ぶことができる。","comment_id":"17","x":6.6103954,"y":9.073737,"p":0.8421722690866182},{"arg_id":"A17_2","argument":"間違えた場合には、戻り方を解説することが重要である。","comment_id":"17","x":6.581857,"y":10.219098,"p":1},{"arg_id":"A18_0","argument":"後期高齢者向けの本は、簡素に説明されているため、非常に読みやすい。","comment_id":"18","x":5.9143023,"y":8.286757,"p":0.590570780712057},{"arg_id":"A23_0","argument":"今の70歳代はまだまだ出来る。","comment_id":"23","x":7.640668,"y":8.403217,"p":1},{"arg_id":"A23_3","argument":"高齢者をターゲットにしているように感じる。","comment_id":"23","x":6.8200336,"y":8.845794,"p":1},{"arg_id":"A23_6","argument":"80，90歳代になってもこのような本を読むかは疑問だ。","comment_id":"23","x":7.5856586,"y":8.302421,"p":1},{"arg_id":"A25_2","argument":"特定の機種では、QRコードが読めない場合があり、解決方法がネットに存在する。","comment_id":"25","x":7.7643585,"y":10.791821,"p":0.9171671155654444},{"arg_id":"A25_4","argument":"初心者向けではなく、少し慣れた人がネットで情報を探すことができる内容である。","comment_id":"25","x":6.5581,"y":11.113135,"p":0},{"arg_id":"A29_0","argument":"注文する前に不安を感じることがある。","comment_id":"29","x":6.8744183,"y":10.477738,"p":0.777455025298317},{"arg_id":"A29_1","argument":"特定の機種に関して不明な部分が存在することがある。","comment_id":"29","x":7.6322775,"y":10.677135,"p":0.9609631213984832},{"arg_id":"A31_1","argument":"特殊詐欺や投資詐欺の巧妙化に注意が必要だと認識している。","comment_id":"31","x":6.4291024,"y":9.738542,"p":0.7398367937506308},{"arg_id":"A31_2","argument":"被害者にならないように努める意識を持っている。","comment_id":"31","x":6.252703,"y":9.799543,"p":0.7340467460842512},{"arg_id":"A33_5","argument":"私はまだ約50％クリアだが、慌てず焦らず、諦めずに進んでいる。","comment_id":"33","x":7.6032205,"y":8.793295,"p":0.8618948792661085},{"arg_id":"A33_6","argument":"時間のある時に少しずつ前に進むことが大切だと感じている。","comment_id":"33","x":6.921975,"y":9.499837,"p":0.6398428929633428},{"arg_id":"A35_0","argument":"便利な機能が多く書かれている本でも、伝わらなければ自己嫌悪に陥ることがある。","comment_id":"35","x":8.009887,"y":10.970514,"p":0.8298997897978274},{"arg_id":"A37_0","argument":"私は70代の母に本を贈りました。","comment_id":"37","x":8.22448,"y":8.513531,"p":0},{"arg_id":"A38_0","argument":"私は70歳を過ぎても理解できる内容だと感じる。","comment_id":"38","x":7.493092,"y":8.572104,"p":1},{"arg_id":"A41_0","argument":"40歳でも十分に役立つ情報が得られる。","comment_id":"41","x":7.194203,"y":8.361037,"p":0.9230662583342338},{"arg_id":"A41_1","argument":"この本は資料性が高く、年齢に関係なく学びがある。","comment_id":"41","x":5.57369,"y":8.443792,"p":0.6035870886945061},{"arg_id":"A43_0","argument":"私はほとんどの事例をすでに知っていたので、購入を早合点してしまったと感じる。","comment_id":"43","x":7.0396576,"y":9.674963,"p":0},{"arg_id":"A44_1","argument":"年齢を重ねると、自分が何を理解していないのかが分からなくなることがある。","comment_id":"44","x":6.9960403,"y":8.963657,"p":1},{"arg_id":"A45_0","argument":"私は色々な操作のやり方を知りたかったが、歳をとった話が多くて期待外れだった。","comment_id":"45","x":7.4989595,"y":9.525,"p":1},{"arg_id":"A45_1","argument":"私は具体的な操作方法を学ぶことを期待していたが、内容がそれに沿っていなかったので残念に思った。","comment_id":"45","x":7.636493,"y":9.696978,"p":1},{"arg_id":"A46_0","argument":"万人にちょうどいい内容を提供するのは難しいと感じる。","comment_id":"46","x":6.1884036,"y":10.573411,"p":1},{"arg_id":"A46_5","argument":"ある程度使える人には物足りなさを感じるかもしれない。","comment_id":"46","x":6.4573226,"y":10.089593,"p":0.8426587837071846},{"arg_id":"A48_0","argument":"私はキャッシュレス決済についての情報をもっと知りたかったが、提供されている情報が限られていると感じる。","comment_id":"48","x":7.4133143,"y":10.040169,"p":1},{"arg_id":"A48_1","argument":"私はPayPay以外のキャッシュレス決済手段についても知りたいと思っている。","comment_id":"48","x":7.4420156,"y":10.0254755,"p":1},{"arg_id":"A48_2","argument":"私はキャッシュレス決済の多様性についての理解が不足していると感じる。","comment_id":"48","x":7.444315,"y":9.921515,"p":1},{"arg_id":"A49_0","argument":"70歳以上の人々の中には、特定の情報や技術に対する理解が不足している場合がある。","comment_id":"49","x":7.155922,"y":8.552295,"p":1},{"arg_id":"A49_1","argument":"返品したいという気持ちは、商品やサービスに対する不満から生じることがある。","comment_id":"49","x":6.6847773,"y":10.299057,"p":1},{"arg_id":"A50_0","argument":"LINEの登録には相手からの認証が必要な場合がある。","comment_id":"50","x":7.1509223,"y":10.929026,"p":1},{"arg_id":"A50_1","argument":"迷惑メールの設定についての情報が重要である。","comment_id":"50","x":7.5841413,"y":11.256763,"p":0.8963653735178357},{"arg_id":"A50_2","argument":"パスワードがわからないとき、パスコードが届かないことがある。","comment_id":"50","x":7.347666,"y":10.731888,"p":1},{"arg_id":"A51_0","argument":"プレゼントした相手がすごく喜んで自慢してきた。","comment_id":"51","x":5.422235,"y":9.738992,"p":0.8846517850013269},{"arg_id":"A51_2","argument":"困っていることが解消されることが喜ばれる。","comment_id":"51","x":5.5667024,"y":10.475273,"p":1},{"arg_id":"A51_3","argument":"自分に合った新しい使い方が見つかることが喜ばれる。","comment_id":"51","x":5.836836,"y":10.266381,"p":1},{"arg_id":"A51_4","argument":"子供に「こんなことも分からないの？」と言われなくなることが喜ばれる。","comment_id":"51","x":5.9341764,"y":10.204947,"p":0},{"arg_id":"A52_4","argument":"シニアの不安要素を解決する方法が示されている。","comment_id":"52","x":5.8023343,"y":8.921027,"p":1},{"arg_id":"A52_7","argument":"デジタル時代の専門用語がシニアの経験に基づいた言葉に置き換えられている。","comment_id":"52","x":6.070187,"y":8.964514,"p":0.8893776666849208},{"arg_id":"A52_8","argument":"著者は20年以上の経験を持ち、シニアに向き合ってきた。","comment_id":"52","x":6.260176,"y":8.642977,"p":0.646208931612379},{"arg_id":"A53_1","argument":"著者の増田先生はシニア世代と接しているため、実践的な内容が提供されている。","comment_id":"53","x":5.8852677,"y":8.694383,"p":1},{"arg_id":"A53_3","argument":"シニア世代のリアルな体験談が共感や新しい発見を促す。","comment_id":"53","x":5.9881973,"y":8.781553,"p":1}]},{"cluster":"使いやすさと理解しやすさ","cluster_id":"0","takeaways":"参加者の意見は、書籍の内容に対する評価が分かれています。多くの人が内容のわかりやすさや視覚的な補助（写真や図）を評価し、親切丁寧な説明が理解を助けると感じています。一方で、索引の欠如や内容の物足りなさ、難解さを指摘する声もありました。\n\n全体として、読みやすさや構成の良さが強調される一方で、さらなる充実を求める意見も見受けられます。既に知識がある読者には物足りなさを感じる部分があり、内容の深さに対する期待が示されています。","arguments":[{"arg_id":"A2_0","argument":"使い道や困りごとが索引になっている点が良い。","comment_id":"2","x":5.1182237,"y":11.192371,"p":0},{"arg_id":"A3_0","argument":"この本はやさしく、丁寧に分かりやすく書かれている。","comment_id":"3","x":3.5789306,"y":10.961764,"p":1},{"arg_id":"A3_1","argument":"この本は優れた内容を持っている。","comment_id":"3","x":3.409111,"y":10.171141,"p":0.8168539383493483},{"arg_id":"A6_0","argument":"この本は基本から解りやすく説明している。","comment_id":"6","x":3.6522377,"y":11.113901,"p":0.5886459506242002},{"arg_id":"A7_0","argument":"私はこの本を使いこなせるようになった。","comment_id":"7","x":3.3956785,"y":9.270017,"p":0.9332438495886864},{"arg_id":"A9_1","argument":"索引が無いので、この本は使いにくいと感じる。","comment_id":"9","x":4.0068665,"y":10.2924595,"p":0.7786359301693038},{"arg_id":"A10_0","argument":"この本は非常に良かったです。","comment_id":"10","x":3.2818668,"y":9.979671,"p":1},{"arg_id":"A10_1","argument":"内容がわかりやすく、興味を引きました。","comment_id":"10","x":4.272762,"y":11.781682,"p":1},{"arg_id":"A10_2","argument":"読みやすい文章で、すぐに引き込まれました。","comment_id":"10","x":4.1594057,"y":12.003611,"p":0.945739649638884},{"arg_id":"A10_3","argument":"全体的に満足のいく体験でした。","comment_id":"10","x":5.2105627,"y":9.869944,"p":1},{"arg_id":"A11_0","argument":"易しい文章は理解しやすく、復習に最適である。","comment_id":"11","x":4.371473,"y":11.960884,"p":1},{"arg_id":"A11_1","argument":"写真があることで、視覚的に情報を補完できる。","comment_id":"11","x":5.4610496,"y":11.763461,"p":1},{"arg_id":"A11_2","argument":"この本は復習を効果的に行うための良いリソースである。","comment_id":"11","x":3.7650893,"y":10.6965065,"p":1},{"arg_id":"A15_2","argument":"初心者にはこの本の内容が理解できない可能性が高い。","comment_id":"15","x":5.5619636,"y":10.73251,"p":0},{"arg_id":"A16_0","argument":"この本は家族に内容を教えるのに役立つ。","comment_id":"16","x":3.8704677,"y":10.641121,"p":0.660816937319749},{"arg_id":"A16_1","argument":"私はこの本が参考になると感じた。","comment_id":"16","x":3.384559,"y":9.424571,"p":0.8497331391134264},{"arg_id":"A18_1","argument":"項目ごとに整理されているので、必要な情報を探しやすい。","comment_id":"18","x":5.2672358,"y":12.201703,"p":0},{"arg_id":"A18_2","argument":"困り事が分かりやすく説明されているため、理解しやすい。","comment_id":"18","x":4.8770957,"y":11.534462,"p":0},{"arg_id":"A19_0","argument":"この本は優しくて分かりやすい内容で、助けられたと感じる。","comment_id":"19","x":3.3984323,"y":10.680363,"p":0.5506872735630878},{"arg_id":"A20_0","argument":"私はこの本の内容の半分ほどをすでに知っていましたが、初めて知ったこともあり、役に立ちました。","comment_id":"20","x":3.612908,"y":9.096841,"p":1},{"arg_id":"A21_0","argument":"この本はタイトルが示すほど易しくはない。","comment_id":"21","x":3.7003324,"y":10.232794,"p":1},{"arg_id":"A21_1","argument":"本書には多くの未解決の問題が残されている。","comment_id":"21","x":4.4852138,"y":10.704371,"p":1},{"arg_id":"A22_0","argument":"文字が大きくて読みやすい。","comment_id":"22","x":3.7077713,"y":12.077973,"p":0.8088483226690135},{"arg_id":"A22_1","argument":"親切丁寧な内容である。","comment_id":"22","x":4.96805,"y":10.3425665,"p":0},{"arg_id":"A22_2","argument":"わかりやすい説明がされている。","comment_id":"22","x":4.951569,"y":11.925593,"p":1},{"arg_id":"A23_1","argument":"この本は薄っぺらで内容がない。","comment_id":"23","x":3.7258167,"y":10.136287,"p":0.9423460659984736},{"arg_id":"A23_2","argument":"1,650円（税込み）は驚くべき価格だ。","comment_id":"23","x":5.3506365,"y":10.118323,"p":0.9949936926661648},{"arg_id":"A23_4","argument":"本の内容は茶飲み話が半分を占めている。","comment_id":"23","x":3.9289937,"y":9.799028,"p":0},{"arg_id":"A24_0","argument":"この本は非常に分かりやすい内容で、初心者でも理解しやすい。","comment_id":"24","x":3.3157768,"y":11.035838,"p":0.567797270608892},{"arg_id":"A24_1","argument":"説明が明確で、難しい概念も簡単に理解できるように工夫されている。","comment_id":"24","x":4.881216,"y":11.916125,"p":1},{"arg_id":"A24_2","argument":"図や例が豊富で、視覚的に理解を助けてくれる。","comment_id":"24","x":5.2150636,"y":11.798983,"p":1},{"arg_id":"A24_3","argument":"文章がシンプルで、専門用語が少ないため、スムーズに読み進められる。","comment_id":"24","x":4.221423,"y":12.232971,"p":0.7621669382471801},{"arg_id":"A24_4","argument":"全体的に構成が良く、段階的に学べるようになっている。","comment_id":"24","x":5.045677,"y":11.654009,"p":1},{"arg_id":"A26_1","argument":"この本は、私にはすでに知識があったため、少し物足りなさを感じた。","comment_id":"26","x":3.7537713,"y":9.149649,"p":1},{"arg_id":"A26_2","argument":"本の内容は気楽に読めるもので、力がつけば推測できるようになる。","comment_id":"26","x":3.9917173,"y":11.323606,"p":0},{"arg_id":"A27_0","argument":"この本は内容が難解で理解しづらい。","comment_id":"27","x":3.938324,"y":10.282651,"p":1},{"arg_id":"A30_0","argument":"この本は、なんとなくの感覚を確実なものにしてくれる。","comment_id":"30","x":3.1394448,"y":10.695461,"p":1},{"arg_id":"A30_1","argument":"内容がわかりやすく、理解しやすい。","comment_id":"30","x":4.4684043,"y":12.127822,"p":0.7621669382471801},{"arg_id":"A30_2","argument":"読んでいると、自信を持って物事を判断できるようになる。","comment_id":"30","x":4.42142,"y":11.190393,"p":0.5470870338480566},{"arg_id":"A32_0","argument":"この本は読みやすいと感じる。","comment_id":"32","x":3.048864,"y":11.18352,"p":0},{"arg_id":"A33_3","argument":"本書の手順に従って一つ一つ確認していけば、操作がクリアできる。","comment_id":"33","x":5.926574,"y":12.033929,"p":1},{"arg_id":"A33_4","argument":"全ての道はホームに通じるので、ホーム画面に戻って操作を確認している。","comment_id":"33","x":6.0858326,"y":12.097393,"p":1},{"arg_id":"A34_0","argument":"確認のために便利なツールがあると、知識を再確認できる。","comment_id":"34","x":5.8707204,"y":11.857971,"p":1},{"arg_id":"A34_1","argument":"知っていることでも、確認することで理解が深まる。","comment_id":"34","x":5.8061366,"y":11.437292,"p":0},{"arg_id":"A35_2","argument":"この本は本当に必要な情報を分かりやすく教えてくれる。","comment_id":"35","x":3.6797848,"y":10.820952,"p":1},{"arg_id":"A35_3","argument":"文字が大きくて読みやすい。","comment_id":"35","x":3.6590505,"y":12.143131,"p":1},{"arg_id":"A36_0","argument":"この本は私にとってまったく役に立たなかった。","comment_id":"36","x":3.5860193,"y":9.558101,"p":0.7980656623294309},{"arg_id":"A36_1","argument":"内容は私がすでに知っていることばかりだった。","comment_id":"36","x":3.8144135,"y":9.021787,"p":0.9551073949474248},{"arg_id":"A37_1","argument":"図書館で一度その本を読んだことがあります。","comment_id":"37","x":3.3287892,"y":9.173241,"p":0.79775218731579},{"arg_id":"A37_2","argument":"その本は文字が大きく、説明が分かりやすいです。","comment_id":"37","x":3.571602,"y":11.415066,"p":0},{"arg_id":"A39_0","argument":"内容が分かりやすいと、読者は理解しやすくなる。","comment_id":"39","x":4.1663485,"y":11.937088,"p":1},{"arg_id":"A44_0","argument":"私はこの本が初歩的な知識を得るのに最適だと感じる。","comment_id":"44","x":3.613377,"y":9.302217,"p":0.9905561503694308},{"arg_id":"A44_2","argument":"この本の内容は、私にとってちょうど良いレベルだと思う。","comment_id":"44","x":3.2235348,"y":9.667761,"p":1},{"arg_id":"A44_3","argument":"それ以上の知識は、自分で学んでいけば良いと考える。","comment_id":"44","x":4.154601,"y":9.094833,"p":0},{"arg_id":"A46_1","argument":"もう少し内容が充実していても良かったと思う。","comment_id":"46","x":4.702581,"y":9.884985,"p":1},{"arg_id":"A46_2","argument":"字が大きく、スペースが広いため、読みやすさはあると感じる。","comment_id":"46","x":3.666619,"y":12.173501,"p":1},{"arg_id":"A46_3","argument":"内容的にはもう一歩進んでほしいと思う。","comment_id":"46","x":4.6465864,"y":10.082296,"p":1},{"arg_id":"A46_6","argument":"続編や上級編があれば良いと考える。","comment_id":"46","x":4.6834927,"y":9.8542185,"p":1},{"arg_id":"A47_1","argument":"本の題名に惹かれて購入したが、中身を試し見できなかったのが残念です。","comment_id":"47","x":4.233997,"y":10.069929,"p":0},{"arg_id":"A51_6","argument":"プレゼントして喜んでもらえたことが嬉しかったので、評価は★5である。","comment_id":"51","x":5.1535892,"y":9.914183,"p":1},{"arg_id":"A52_1","argument":"著者の温かいメッセージが感じられる自己啓発書である。","comment_id":"52","x":3.1261234,"y":10.436752,"p":1},{"arg_id":"A52_3","argument":"ふんだんな活用事例が紹介されている。","comment_id":"52","x":5.3840427,"y":11.37526,"p":1},{"arg_id":"A52_5","argument":"各種設定や操作方法が項目毎に図解されている。","comment_id":"52","x":5.4621377,"y":12.184752,"p":0},{"arg_id":"A52_6","argument":"読みやすい大きな文字とレイアウトで編集されている。","comment_id":"52","x":3.6193311,"y":12.243086,"p":0.7822074911497644},{"arg_id":"A52_9","argument":"この本は著者の秀逸さを感じることができるシリーズの最新作である。","comment_id":"52","x":3.087457,"y":10.275312,"p":0},{"arg_id":"A54_0","argument":"この本は、解決したい困り事が多く掲載されている。","comment_id":"54","x":4.6434326,"y":10.770651,"p":1},{"arg_id":"A54_1","argument":"行間隔が広く、読みやすいデザインになっている。","comment_id":"54","x":3.8416157,"y":12.1005745,"p":0.4488007055320658},{"arg_id":"A54_2","argument":"イラストが豊富で、内容がわかりやすく解説されている。","comment_id":"54","x":4.841203,"y":11.979522,"p":1},{"arg_id":"A54_3","argument":"ページをめくることで、読みたい情報を簡単に探せる。","comment_id":"54","x":4.9994345,"y":12.393926,"p":0},{"arg_id":"A54_4","argument":"困り事を理解する楽しさを提供している本である。","comment_id":"54","x":4.726085,"y":10.801665,"p":1}]},{"cluster":"スマホ操作の機種別情報不足","cluster_id":"1","takeaways":"参加者は、スマホの操作に関する本が高齢者にとって役立つと評価していますが、内容がiPhoneに偏っていると感じています。特に、Androidユーザーにとっては情報が不足しており、操作方法が異なることが多いと指摘されています。\n\nまた、初心者向けのわかりやすい説明が好評ですが、特定の状況への対処法が中心で、基本的な操作方法が不足しているとの意見もあります。全体として、この本は高齢者がスマホを活用するための実用的なガイドとして評価されていますが、より多様な機種に対応した内容が求められています。","arguments":[{"arg_id":"A4_0","argument":"私はアンドロイドを使用しているので、レビューが参考になるかどうかは微妙だと感じる。","comment_id":"4","x":9.763085,"y":10.668023,"p":1},{"arg_id":"A12_0","argument":"アンドロイドとアイホンの両方の特徴を説明していることが良い。","comment_id":"12","x":9.834531,"y":10.692378,"p":1},{"arg_id":"A13_0","argument":"iPhoneに特化した内容が多く、Androidユーザーには不満が残る。","comment_id":"13","x":9.761358,"y":10.414989,"p":0.4072930194676928},{"arg_id":"A13_1","argument":"レビューがiPhoneの機能や利点に偏っているため、他のスマートフォンユーザーには参考になりにくい。","comment_id":"13","x":9.563364,"y":10.46595,"p":0.2924348124898321},{"arg_id":"A17_0","argument":"アンドロイドとアイフォンの解説が別々にされているが、機種によって画面が大きく異なるため、お年寄りには理解が難しい。","comment_id":"17","x":9.877472,"y":10.501099,"p":0.5747535826235006},{"arg_id":"A23_5","argument":"アンドロイドとアイフォン版があり、実質的に内容が1/4になっている。","comment_id":"23","x":9.808647,"y":10.613427,"p":1},{"arg_id":"A25_0","argument":"この本は、スマホに慣れた人にとっては当たり前の内容が多い。","comment_id":"25","x":8.884115,"y":10.281534,"p":0.9442572432618448},{"arg_id":"A25_1","argument":"Androidスマホでは、バージョンやメーカーによって画面が異なることがある。","comment_id":"25","x":9.712941,"y":10.21128,"p":0.281314415386357},{"arg_id":"A25_3","argument":"余分な文章が多く、Android機種に対する比較検討が不十分である。","comment_id":"25","x":9.42508,"y":10.77225,"p":0.2348973900130701},{"arg_id":"A26_0","argument":"高齢者がスマホを使いこなすことは非常に便利で、頭の運動にもなる。","comment_id":"26","x":8.782173,"y":9.385458,"p":1},{"arg_id":"A26_3","argument":"全体的に、スマホの使い方を学ぶのに適した本だと思う。","comment_id":"26","x":8.937648,"y":9.958264,"p":1},{"arg_id":"A28_0","argument":"80歳を過ぎた知り合いのためにスマホを購入したが、操作が難しいと感じる。","comment_id":"28","x":8.431045,"y":8.896814,"p":1},{"arg_id":"A31_0","argument":"スマホ初心者にとって、説明がわかりやすいと感じる。","comment_id":"31","x":8.709556,"y":10.094169,"p":0.8854239851382834},{"arg_id":"A31_3","argument":"少しずつ覚えてスマホをマスターしていきたいと考えている。","comment_id":"31","x":8.91966,"y":9.605947,"p":0},{"arg_id":"A33_0","argument":"私は78歳で、初めて本書を使ってスマホの操作を学んでいる。","comment_id":"33","x":8.508866,"y":8.9204,"p":0.5816073279148519},{"arg_id":"A33_1","argument":"本書は世界一簡単なスマホの操作を教えてくれる。","comment_id":"33","x":8.672992,"y":9.9199705,"p":1},{"arg_id":"A33_2","argument":"私は120ページで、70歳からのスマホのつまづき原因ベスト3を確認している。","comment_id":"33","x":8.390336,"y":8.841068,"p":1},{"arg_id":"A35_1","argument":"生活に必要な機能はそれほど多くはないと感じる。","comment_id":"35","x":8.314469,"y":10.829595,"p":0.8298997897978274},{"arg_id":"A37_3","argument":"母は手元に置いておきたいと言っています。","comment_id":"37","x":8.781524,"y":8.680019,"p":0},{"arg_id":"A40_0","argument":"スマホの多様な使い方についての情報が非常に参考になる。","comment_id":"40","x":9.19917,"y":10.151245,"p":0},{"arg_id":"A40_1","argument":"この本はスマホを活用するための実用的なガイドである。","comment_id":"40","x":8.804781,"y":9.877519,"p":1},{"arg_id":"A42_0","argument":"私は70代の父親にスマホを使わせるためにこの本を購入しました。","comment_id":"42","x":8.427108,"y":8.834368,"p":1},{"arg_id":"A42_1","argument":"父親と一緒に暮らしていないので、使い方を教えるのが難しいです。","comment_id":"42","x":8.95115,"y":8.8106,"p":0},{"arg_id":"A42_2","argument":"父親は元々パソコンが好きなので、独力でスマホをマスターしてくれることを期待しています。","comment_id":"42","x":8.902143,"y":9.138886,"p":0},{"arg_id":"A46_4","argument":"アプリのまとめ方は書いてあるが、並べ替えの仕方が書いてないのは残念だ。","comment_id":"46","x":8.942228,"y":10.825217,"p":0},{"arg_id":"A47_0","argument":"私は、スマホの操作が分からない母のためにこの本を購入しました。","comment_id":"47","x":8.640092,"y":9.093135,"p":0.2594958039729968},{"arg_id":"A47_2","argument":"私は基本的なスマホの操作方法が載っていると思っていましたが、実際は特定の状況に対する対処法が中心でした。","comment_id":"47","x":9.056295,"y":10.2387295,"p":1},{"arg_id":"A47_3","argument":"ある程度のスマホ知識がある人には理解できる内容だと思いますが、初心者には難しいと感じます。","comment_id":"47","x":8.402456,"y":10.293341,"p":0},{"arg_id":"A51_1","argument":"スマホがうまく使えない人にとって、細かい機能や仕組みは重要ではない。","comment_id":"51","x":9.126167,"y":10.496492,"p":0},{"arg_id":"A51_5","argument":"この本は「技術書」ではなく「生活に、うまく使える小技集」のような内容である。","comment_id":"51","x":8.482986,"y":10.628066,"p":0.8089905482075269},{"arg_id":"A52_0","argument":"本書は高齢者がスマホを活用することでシニアライフに活力と楽しみをもたらすことを伝える。","comment_id":"52","x":8.703363,"y":9.351507,"p":1},{"arg_id":"A52_2","argument":"内容はスマホを生活のパートナーとするマインドを提案している。","comment_id":"52","x":9.21862,"y":9.795726,"p":1},{"arg_id":"A53_0","argument":"スマホの使い方はシニア世代に特有の視点が必要である。","comment_id":"53","x":9.24793,"y":9.600071,"p":0},{"arg_id":"A53_2","argument":"具体的なスマホの使い方がわからない人にとって、この本は役立つ情報を提供する。","comment_id":"53","x":8.945184,"y":10.34976,"p":1},{"arg_id":"A53_4","argument":"スマホの使い方を学ぶことで、日常生活が便利になる可能性がある。","comment_id":"53","x":9.105534,"y":9.832881,"p":1}]}],"comments":{"1":{"comment":"やはり、高齢者には難語句が多くて困った、"},"2":{"comment":"使い道、困りごとが索引になっている点が良い。"},"3":{"comment":"やさしく、かつ丁寧に分かりやすく書いてあり優れものです。"},"4":{"comment":"使用しているのが、アンドロイドなので、参考になるような、ならないようなモドカシサ😅"},"5":{"comment":"私が知りたいことは一つも見いだせなかったのです。これが通販のむつかしいところですね。"},"6":{"comment":"基本から解りやすく高齢者にはピッタリと感じました。"},"7":{"comment":"使いこなせる様になった。"},"8":{"comment":"知りたいことが理解できない場合が多い"},"9":{"comment":"使い方がわからない時に使っています。索引が無いので使いにくい。"},"10":{"comment":"よかったです"},"11":{"comment":"易しい文章と写真で復習に最適です。"},"12":{"comment":"アンドロイドとアイホンの双方を説明しているのが良い。"},"13":{"comment":"iPhone使用者に傾いた内容で、Android使用者の私には不満足なところがあった。"},"14":{"comment":"もっと驚くような使い方が有るかと期待したが、あまり私には参考にはならなかった。"},"15":{"comment":"内容は一応メールとWeb程度は出来る人向け。まったくの初心者には猫に小判。"},"16":{"comment":"家の親に内容を教えてあげられる、良い本でした。参考になりました"},"17":{"comment":"アンドロイドとアイフォン別に解説はしてあるが機種によってかなり画面が違って来るのでお年寄りには理解困難。やはり触りながら間違えながら覚えて行くのが頭に入る。間違えたら戻り方を解説するとか…。"},"18":{"comment":"後期高齢者の主人が購入簡素に説明されているので読みやすい項目毎に探し安く困り事が分かり易い"},"19":{"comment":"優しくて分かり易いので助かりました。"},"20":{"comment":"半分ほどはすでに知っていることばかりでしたが、初めて知ったこともあってまあ、役に立ちました。"},"21":{"comment":"タイトル程易しくはない。未解決がたくさん残る。"},"22":{"comment":"文字も大きく、読み手側に立っていて、親切丁寧、わかりやすかったです。"},"23":{"comment":"今の70歳代はまだまだ出来ます。こんな薄っぺらで、内容のない本が1,650円（税込み）とは驚きです。まるで「高齢者を餌にして釣りあげようとしている」は言い過ぎでしょうか。茶飲み話が半分で、アンドロイドとアイフォン版があり実質１/4の量です。そもそも80，90歳代になればこういった本を読むのかどうか分かりませんが。"},"24":{"comment":"分かりやすい"},"25":{"comment":"表題からすると期待できる内容ですが少しスマホ使い慣れた人にとっては当たり前の内容でした。androidスマホではバージョンが異なると書いてある通りの画面委ならない、メーカーによっては全く該当項目が隠れており見つからない、極端の例ではK国の機種では QRコードが読めず、解決方法はLINEの機能で読み取る必要があるなどネットにもいろいろと解決方法が発表されている。余分な文章が多く、android機種に対する記事の内容が比較検討が不十分である。少し慣れた人ならネットで探すなど補えるが、初心者向けではない本である。"},"26":{"comment":"高齢者がスマホを使いこなせたら、とても便利で頭の運動になると思いますが、残念ながら私には書いてあることのおおよそはマスターしていましたので少し物足りなかったか？というところで、星3つです力がつけば推測できたりできるようになりますから、その点では気楽に読める本だと思います"},"27":{"comment":"分かりにくい。"},"28":{"comment":"80歳過ぎた知り合いの為購入しました中々スマホの操作は難しいですね"},"29":{"comment":"注文する前から不安でしたが、やはり私の機種では不明な部分が有りました。"},"30":{"comment":"なんとなくが確実になる、しかもわかりやすい！"},"31":{"comment":"私のようなスマホ初心者にとっては、説明が比較的わかりやすくなっていて大変勉強になります。なお、昨今は特殊詐欺や投資詐欺といった巧妙化した事象が報道されて、注意しなければならない事項などもあり、被害者とならないように努める所存です。これからも時折読みながら、少しずつ覚えてマスターしていきたいと考えている。"},"32":{"comment":"今読んでいるが解借りやすいです"},"33":{"comment":"私は、７８歳になりますが、初めて本書により世界一簡単なスマホの操作と言う事で一つずつ本を見ながら、操作の仕方を確認しています。只今、１２０頁で７０歳からのスマホ「つまづき原因ベスト３」、この本に従って一つ一つ確認していけばクリア－出来ます。全ての道は「ホ－ム」に通じるでホ－ム画面に戻り操作して本書の手順に添って確認して行けば解ってきます。私はまだ約５０％クリア－ですが、慌てず、焦らず、諦めないで時間のある時に少しづつ前に進んでいます。"},"34":{"comment":"知ってていても確認…等のために便利です"},"35":{"comment":"便利な機能をたくさん書いている本を読んでも、伝わらなければ、自己嫌悪に落ちいるだけ。おまけに生活な必要な機能は、そんなにたくさんは要らない。この本は本当に必要な情報を分かりやすく教えてくれます。文字も大きいので読みやすい。"},"36":{"comment":"70歳女性。まったく役にたたなかった。知っている事しか書いてありませんでした。"},"37":{"comment":"７０代の母に贈答しました。図書館で借りてきて一度拝読はしたのですが文字が大きく説明も分かりやすいらしくて手元に置いておきたいとの事でしたので購入しました。"},"38":{"comment":"70歳をすぎた私でも理解できる内容でした。"},"39":{"comment":"内容が分かりやすく、理解しやすいです。"},"40":{"comment":"スマホを色々に使いまくる本で、おおいに参考になりました。"},"41":{"comment":"40歳でも充分過ぎる資料性高い！"},"42":{"comment":"70代でスマホデビューした父親に購入しました。一緒に暮らしていないので、なかなか使い方を教えてあげられないのでこちらの本を送りました。元々パソコンなどは好きなので独力でマスターしてもらおうと思います。"},"43":{"comment":"ただほとんどの事例はすでに知っていたので、ちょっと早合点で買ってしまったかな、と思いました。"},"44":{"comment":"小難しい事は全然ないし、初歩の初歩知識が、得られ満足です。年齢が上がると何が判らないかが判らないのでこれぐらいでちょうど良い。それから先は各々で知識を得れば良いと思います。"},"45":{"comment":"色々な操作のやり方を知りたかったが、歳をとった話しとかが多く期待外れで残念。"},"46":{"comment":"多分、万人にちょうどいい、というのは難しいのだと思いますが、もうちょっと内容があっても良かったかな？よく使い方について両親が聞いてくるので、参考になれば、と購入。字が大きく、スペースも広くて読みやすさはあると思います。ただ、内容的にはもうひと超えあっても良いかも。アプリのまとめ方は書いてあっても、並べ替えの仕方は書いてない、など、ちょっと残念。ある程度使える人なら、物足りなさはあると思います。続編、上級編などあったら良いかしら？"},"47":{"comment":"スマホの操作が分からないと悩む母へ、本の題名につられ購入しました。本の購入時、中身を試し見が出来なかったので、題名とレビューを参考に購入しました。基本的なスマホの操作方法が載っているのかと思ったら、こういう場合はこういう対処をすると言う内容の物が主です。ある程度スマホの知識がある方なら理解出来る内容だと思いますが、初心者がこちらの本を見て理解するには難しいと思います。"},"48":{"comment":"大体、知っている事ばかりでした。キャッシュレスの事を知りたかったのですが、PayPayだけですか?他には無いのでしょうか?"},"49":{"comment":"このようなことが分からない70歳以上の人がどれほどいるのかな? 返品したい気持ち!"},"50":{"comment":"LINEの登録は相手からの認証が必要？迷惑メールの設定について知りたかった、パスワードが解らない時パスコ-ドが届かない。"},"51":{"comment":"知り合いの人にプレゼントしたところ、次に会った時にすごく喜んで自慢をしてきました笑スマホがうまく使えない人にとって・スマホの細かい機能、仕組みなんかはどうでもよくて、・困っていることが解消したり・自分にあった新しい使い方が見つかったり・子供に聞いて「こんなことも分からないの？」と言われなくなるみたいなことの方が喜ばれるんだな、と感じました笑これは「技術書」じゃなくて「生活に、うまく使える小技集」みたいな感じですかねプレゼントしたら喜んでもらえて嬉しかったので★5です！"},"52":{"comment":"本書はスマホ活用のハウツー書に留まらず、高齢者がスマホを活用することで、シニアライフに活力と楽しみを享受できる素晴らしさを伝えたい。そんな著者からのシニアに寄り添う温かいメッセージを感じる自己啓発書とも言える。内容は・スマホを生活のパートナーとするマインド・ふんだんな活用事例・シニアの不安要素の解決・項目毎の図解による各種設定や操作方法これらを読みやすい大きな文字とレイアウトで編集され、デジタル時代の専門用語はシニアが経験してきた時代背景を考慮した言葉に置き換えて書かれている。20年以上自身のパソコン教室でシニアに向き合ってきた、著者ならではの秀逸さを感じることができたシリーズ最新本です。"},"53":{"comment":"スマホの使い方だけでなくシニアだからこその使い方、スマホとの向き合い方がわかります。普段からシニア世代と接している著者増田先生だからこそ…の内容です。スマホでいろいろできると言われても具体的に何にどう使ったらよいのかわからない、人にも聞けない、聞き方がわからないという方は一通り読んで試してみるのもよいでしょう。シニア世代のリアルな体験談も載っています「そうそうこういう時困る！」「へぇこういう風に使っている人もいるんだ」と共感したり、新しい発見をしたりすることができるでしょう。"},"54":{"comment":"表紙のタイトル通り、人に聞かなくても、解決したい困り事あるあるが載ってます！行間隔が広いので読みやすいし、イラストでわかりやすく解説してありますよ。パラパラめくって、読みたいところがパッと探しやすいです。困り事が、わかる楽しさに変わる本です。"}},"translations":{},"overview":"公的な意見収集の結果、高齢者向けの情報提供には簡潔でわかりやすい表現が求められ、実践的な学習が効果的であることが強調されました。書籍の内容については、視覚的補助が評価される一方で、索引の欠如や深さの不足が指摘され、特に既存の知識を持つ読者には物足りなさが感じられました。また、スマホ操作に関してはiPhoneに偏った情報が多く、Androidユーザー向けの内容が不足しているとの意見があり、より多様な機種への対応が求められています。","config":{"name":"世界一簡単！ 70歳からのスマホの使いこなし術","question":"『世界一簡単！ 70歳からのスマホの使いこなし術』について、読者からの主なフィードバックは何ですか？","input":"review","model":"gpt-4o-mini","extraction":{"workers":3,"limit":100,"prompt_file":"review","source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nあなたは、テキスト内から主要な議論を抽出する議論抽出者です。\n入力テキストは、Amazonの本のレビューで構成されています。\n各メッセージには、議論を含む形式でPythonのリストとして回答します。\n各議論は、それ自体で理解可能で読みやすいものである必要があります。\n議論は、他の議論や元のテキストを参照せず、一人称で表現し、一般的な真実を現在形で述べます。","model":"gpt-4o-mini"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"intro":"2025年2月9日時点で収集された54件のレビューに基づく、顧客レビューの分析結果を以下に示します。","output_dir":"review","previous":{"name":"Smartphone Usage Guide for Seniors","question":"What are the main points of feedback from readers about the book '世界一簡単！ 70歳からのスマホの使いこなし術'?","input":"review","model":"gpt-4o-mini","extraction":{"workers":3,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"translation":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."},"intro":"The following is an analysis of customer reviews for the book '世界一簡単！ 70歳からのスマホの使いこなし術'. This summary is based on 54 reviews collected as of 2025-02-09.","output_dir":"review","embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"completed","start_time":"2025-02-09T13:09:45.469843","completed_jobs":[{"step":"extraction","completed":"2025-02-09T13:10:17.168788","duration":31.69538,"params":{"workers":3,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-02-09T13:10:21.788692","duration":4.618299,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2025-02-09T13:10:31.708968","duration":9.919167,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2025-02-09T13:10:33.441345","duration":1.73174,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-02-09T13:10:43.065151","duration":9.621606,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-02-09T13:10:45.727661","duration":2.658937,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"}},{"step":"translation","completed":"2025-02-09T13:11:52.793903","duration":67.062177,"params":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."}},{"step":"aggregation","completed":"2025-02-09T13:11:52.830985","duration":0.034075,"params":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"}},{"step":"visualization","completed":"2025-02-09T13:12:00.454150","duration":7.621994,"params":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"}}],"lock_until":"2025-02-09T13:17:00.455570","current_job":"visualization","current_job_started":"2025-02-09T13:11:52.832213","translation_prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.","previously_completed_jobs":[],"end_time":"2025-02-09T13:12:00.455564"},"embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"},"translation":{"languages":[],"flags":[],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit, prompt"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"running","start_time":"2025-02-09T15:49:15.440124","completed_jobs":[{"step":"extraction","completed":"2025-02-09T15:49:55.533485","duration":40.089784,"params":{"workers":3,"limit":100,"prompt_file":"review","source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nあなたは、テキスト内から主要な議論を抽出する議論抽出者です。\n入力テキストは、Amazonの本のレビューで構成されています。\n各メッセージには、議論を含む形式でPythonのリストとして回答します。\n各議論は、それ自体で理解可能で読みやすいものである必要があります。\n議論は、他の議論や元のテキストを参照せず、一人称で表現し、一般的な真実を現在形で述べます。","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-02-09T15:49:57.609041","duration":2.074405,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2025-02-09T15:50:07.793381","duration":10.182643,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2025-02-09T15:50:09.799764","duration":2.004841,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-02-09T15:50:17.331897","duration":7.530286,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-02-09T15:50:20.605403","duration":3.271323,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"}},{"step":"translation","completed":"2025-02-09T15:50:20.609656","duration":0.002174,"params":{"languages":[],"flags":[],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.","model":"gpt-4o-mini"}}],"lock_until":"2025-02-09T15:55:20.611493","current_job":"aggregation","current_job_started":"2025-02-09T15:50:20.611487"}}},"__N_SSG":true},"page":"/","query":{},"buildId":"9jyuozYdQoy50oWM2XFRY","assetPrefix":".","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>