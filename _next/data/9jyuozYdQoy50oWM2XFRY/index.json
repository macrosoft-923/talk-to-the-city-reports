{"pageProps":{"result":{"clusters":[{"cluster":"é«˜é½¢è€…å‘ã‘ã®ç†è§£ã®é›£ã—ã•","cluster_id":"2","takeaways":"é«˜é½¢è€…å‘ã‘ã®æƒ…å ±æä¾›ã«ã¯ã€ç°¡æ½”ã§ã‚ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚å¤šãã®å‚åŠ è€…ãŒã€åˆå¿ƒè€…ã«ã¯é›£ã—ã„å†…å®¹ã‚„ç‰¹å®šã®æ©Ÿç¨®ã«é–¢ã™ã‚‹ä¸æ˜ç‚¹ã‚’æŒ‡æ‘˜ã—ã¾ã—ãŸã€‚ã¾ãŸã€å®Ÿéš›ã«æ“ä½œã—ãªãŒã‚‰å­¦ã¶ã“ã¨ãŒåŠ¹æœçš„ã§ã‚ã‚Šã€é–“é•ãˆãŸéš›ã®ãƒ•ã‚©ãƒ­ãƒ¼ãŒé‡è¦ã ã¨å¼·èª¿ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nä¸€æ–¹ã§ã€70æ­³ä»£ã§ã‚‚ç†è§£ã§ãã‚‹å†…å®¹ã‚’æ±‚ã‚ã‚‹å£°ã‚„ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹æ±ºæ¸ˆã®å¤šæ§˜æ€§ã«ã¤ã„ã¦ã®çŸ¥è­˜ä¸è¶³ã‚’æ„Ÿã˜ã‚‹æ„è¦‹ã‚‚ã‚ã‚Šã¾ã—ãŸã€‚è‘—è€…ã®å®Ÿè·µçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒè©•ä¾¡ã•ã‚Œã‚‹ä¸€æ–¹ã§ã€ä¸‡äººã«åˆã†å†…å®¹ã‚’æä¾›ã™ã‚‹é›£ã—ã•ã‚‚æµ®ãå½«ã‚Šã«ãªã£ã¦ã„ã¾ã™ã€‚","arguments":[{"arg_id":"A1_0","argument":"é«˜é½¢è€…ã«ã¯é›£ã—ã„èªå¥ãŒå¤šãå«ã¾ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"1","x":6.8983545,"y":8.727041,"p":1},{"arg_id":"A5_0","argument":"é€šè²©ã§ã¯ã€å¿…è¦ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"5","x":7.2100425,"y":10.60301,"p":0.8405943239099011},{"arg_id":"A6_1","argument":"é«˜é½¢è€…ã«ã¨ã£ã¦éå¸¸ã«é©ã—ã¦ã„ã‚‹ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"6","x":6.851038,"y":8.654147,"p":1},{"arg_id":"A8_0","argument":"ç§ã¯çŸ¥ã‚ŠãŸã„ã“ã¨ãŒç†è§£ã§ããªã„å ´åˆãŒå¤šã„ã€‚","comment_id":"8","x":7.086517,"y":10.166238,"p":0},{"arg_id":"A9_0","argument":"ä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„æ™‚ã«ã“ã®æœ¬ã‚’ä½¿ã†ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"9","x":7.37387,"y":11.034832,"p":0.91594491264353},{"arg_id":"A14_0","argument":"ç§ã¯ã‚‚ã£ã¨é©šãã‚ˆã†ãªä½¿ã„æ–¹ã‚’æœŸå¾…ã—ã¦ã„ãŸãŒã€ã‚ã¾ã‚Šå‚è€ƒã«ãªã‚‰ãªã‹ã£ãŸã€‚","comment_id":"14","x":7.4562573,"y":9.626232,"p":1},{"arg_id":"A15_0","argument":"ã“ã®æœ¬ã¯ã€ãƒ¡ãƒ¼ãƒ«ã‚„Webã®åŸºæœ¬ãŒã§ãã‚‹äººå‘ã‘ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"15","x":7.331888,"y":11.438185,"p":0},{"arg_id":"A15_1","argument":"ã¾ã£ãŸãã®åˆå¿ƒè€…ã«ã¯å†…å®¹ãŒé›£ã—ãæ„Ÿã˜ã‚‰ã‚Œã‚‹ã€‚","comment_id":"15","x":6.293159,"y":10.848359,"p":1},{"arg_id":"A17_1","argument":"ãŠå¹´å¯„ã‚Šã¯ã€å®Ÿéš›ã«è§¦ã‚ŠãªãŒã‚‰é–“é•ãˆãªãŒã‚‰è¦šãˆã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹æœçš„ã«å­¦ã¶ã“ã¨ãŒã§ãã‚‹ã€‚","comment_id":"17","x":6.6103954,"y":9.073737,"p":0.8421722690866182},{"arg_id":"A17_2","argument":"é–“é•ãˆãŸå ´åˆã«ã¯ã€æˆ»ã‚Šæ–¹ã‚’è§£èª¬ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚","comment_id":"17","x":6.581857,"y":10.219098,"p":1},{"arg_id":"A18_0","argument":"å¾ŒæœŸé«˜é½¢è€…å‘ã‘ã®æœ¬ã¯ã€ç°¡ç´ ã«èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€éå¸¸ã«èª­ã¿ã‚„ã™ã„ã€‚","comment_id":"18","x":5.9143023,"y":8.286757,"p":0.590570780712057},{"arg_id":"A23_0","argument":"ä»Šã®70æ­³ä»£ã¯ã¾ã ã¾ã å‡ºæ¥ã‚‹ã€‚","comment_id":"23","x":7.640668,"y":8.403217,"p":1},{"arg_id":"A23_3","argument":"é«˜é½¢è€…ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã—ã¦ã„ã‚‹ã‚ˆã†ã«æ„Ÿã˜ã‚‹ã€‚","comment_id":"23","x":6.8200336,"y":8.845794,"p":1},{"arg_id":"A23_6","argument":"80ï¼Œ90æ­³ä»£ã«ãªã£ã¦ã‚‚ã“ã®ã‚ˆã†ãªæœ¬ã‚’èª­ã‚€ã‹ã¯ç–‘å•ã ã€‚","comment_id":"23","x":7.5856586,"y":8.302421,"p":1},{"arg_id":"A25_2","argument":"ç‰¹å®šã®æ©Ÿç¨®ã§ã¯ã€QRã‚³ãƒ¼ãƒ‰ãŒèª­ã‚ãªã„å ´åˆãŒã‚ã‚Šã€è§£æ±ºæ–¹æ³•ãŒãƒãƒƒãƒˆã«å­˜åœ¨ã™ã‚‹ã€‚","comment_id":"25","x":7.7643585,"y":10.791821,"p":0.9171671155654444},{"arg_id":"A25_4","argument":"åˆå¿ƒè€…å‘ã‘ã§ã¯ãªãã€å°‘ã—æ…£ã‚ŒãŸäººãŒãƒãƒƒãƒˆã§æƒ…å ±ã‚’æ¢ã™ã“ã¨ãŒã§ãã‚‹å†…å®¹ã§ã‚ã‚‹ã€‚","comment_id":"25","x":6.5581,"y":11.113135,"p":0},{"arg_id":"A29_0","argument":"æ³¨æ–‡ã™ã‚‹å‰ã«ä¸å®‰ã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"29","x":6.8744183,"y":10.477738,"p":0.777455025298317},{"arg_id":"A29_1","argument":"ç‰¹å®šã®æ©Ÿç¨®ã«é–¢ã—ã¦ä¸æ˜ãªéƒ¨åˆ†ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"29","x":7.6322775,"y":10.677135,"p":0.9609631213984832},{"arg_id":"A31_1","argument":"ç‰¹æ®Šè©æ¬ºã‚„æŠ•è³‡è©æ¬ºã®å·§å¦™åŒ–ã«æ³¨æ„ãŒå¿…è¦ã ã¨èªè­˜ã—ã¦ã„ã‚‹ã€‚","comment_id":"31","x":6.4291024,"y":9.738542,"p":0.7398367937506308},{"arg_id":"A31_2","argument":"è¢«å®³è€…ã«ãªã‚‰ãªã„ã‚ˆã†ã«åŠªã‚ã‚‹æ„è­˜ã‚’æŒã£ã¦ã„ã‚‹ã€‚","comment_id":"31","x":6.252703,"y":9.799543,"p":0.7340467460842512},{"arg_id":"A33_5","argument":"ç§ã¯ã¾ã ç´„50ï¼…ã‚¯ãƒªã‚¢ã ãŒã€æ…Œã¦ãšç„¦ã‚‰ãšã€è«¦ã‚ãšã«é€²ã‚“ã§ã„ã‚‹ã€‚","comment_id":"33","x":7.6032205,"y":8.793295,"p":0.8618948792661085},{"arg_id":"A33_6","argument":"æ™‚é–“ã®ã‚ã‚‹æ™‚ã«å°‘ã—ãšã¤å‰ã«é€²ã‚€ã“ã¨ãŒå¤§åˆ‡ã ã¨æ„Ÿã˜ã¦ã„ã‚‹ã€‚","comment_id":"33","x":6.921975,"y":9.499837,"p":0.6398428929633428},{"arg_id":"A35_0","argument":"ä¾¿åˆ©ãªæ©Ÿèƒ½ãŒå¤šãæ›¸ã‹ã‚Œã¦ã„ã‚‹æœ¬ã§ã‚‚ã€ä¼ã‚ã‚‰ãªã‘ã‚Œã°è‡ªå·±å«Œæ‚ªã«é™¥ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"35","x":8.009887,"y":10.970514,"p":0.8298997897978274},{"arg_id":"A37_0","argument":"ç§ã¯70ä»£ã®æ¯ã«æœ¬ã‚’è´ˆã‚Šã¾ã—ãŸã€‚","comment_id":"37","x":8.22448,"y":8.513531,"p":0},{"arg_id":"A38_0","argument":"ç§ã¯70æ­³ã‚’éãã¦ã‚‚ç†è§£ã§ãã‚‹å†…å®¹ã ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"38","x":7.493092,"y":8.572104,"p":1},{"arg_id":"A41_0","argument":"40æ­³ã§ã‚‚ååˆ†ã«å½¹ç«‹ã¤æƒ…å ±ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚","comment_id":"41","x":7.194203,"y":8.361037,"p":0.9230662583342338},{"arg_id":"A41_1","argument":"ã“ã®æœ¬ã¯è³‡æ–™æ€§ãŒé«˜ãã€å¹´é½¢ã«é–¢ä¿‚ãªãå­¦ã³ãŒã‚ã‚‹ã€‚","comment_id":"41","x":5.57369,"y":8.443792,"p":0.6035870886945061},{"arg_id":"A43_0","argument":"ç§ã¯ã»ã¨ã‚“ã©ã®äº‹ä¾‹ã‚’ã™ã§ã«çŸ¥ã£ã¦ã„ãŸã®ã§ã€è³¼å…¥ã‚’æ—©åˆç‚¹ã—ã¦ã—ã¾ã£ãŸã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"43","x":7.0396576,"y":9.674963,"p":0},{"arg_id":"A44_1","argument":"å¹´é½¢ã‚’é‡ã­ã‚‹ã¨ã€è‡ªåˆ†ãŒä½•ã‚’ç†è§£ã—ã¦ã„ãªã„ã®ã‹ãŒåˆ†ã‹ã‚‰ãªããªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"44","x":6.9960403,"y":8.963657,"p":1},{"arg_id":"A45_0","argument":"ç§ã¯è‰²ã€…ãªæ“ä½œã®ã‚„ã‚Šæ–¹ã‚’çŸ¥ã‚ŠãŸã‹ã£ãŸãŒã€æ­³ã‚’ã¨ã£ãŸè©±ãŒå¤šãã¦æœŸå¾…å¤–ã‚Œã ã£ãŸã€‚","comment_id":"45","x":7.4989595,"y":9.525,"p":1},{"arg_id":"A45_1","argument":"ç§ã¯å…·ä½“çš„ãªæ“ä½œæ–¹æ³•ã‚’å­¦ã¶ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ãŸãŒã€å†…å®¹ãŒãã‚Œã«æ²¿ã£ã¦ã„ãªã‹ã£ãŸã®ã§æ®‹å¿µã«æ€ã£ãŸã€‚","comment_id":"45","x":7.636493,"y":9.696978,"p":1},{"arg_id":"A46_0","argument":"ä¸‡äººã«ã¡ã‚‡ã†ã©ã„ã„å†…å®¹ã‚’æä¾›ã™ã‚‹ã®ã¯é›£ã—ã„ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"46","x":6.1884036,"y":10.573411,"p":1},{"arg_id":"A46_5","argument":"ã‚ã‚‹ç¨‹åº¦ä½¿ãˆã‚‹äººã«ã¯ç‰©è¶³ã‚Šãªã•ã‚’æ„Ÿã˜ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚","comment_id":"46","x":6.4573226,"y":10.089593,"p":0.8426587837071846},{"arg_id":"A48_0","argument":"ç§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹æ±ºæ¸ˆã«ã¤ã„ã¦ã®æƒ…å ±ã‚’ã‚‚ã£ã¨çŸ¥ã‚ŠãŸã‹ã£ãŸãŒã€æä¾›ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"48","x":7.4133143,"y":10.040169,"p":1},{"arg_id":"A48_1","argument":"ç§ã¯PayPayä»¥å¤–ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹æ±ºæ¸ˆæ‰‹æ®µã«ã¤ã„ã¦ã‚‚çŸ¥ã‚ŠãŸã„ã¨æ€ã£ã¦ã„ã‚‹ã€‚","comment_id":"48","x":7.4420156,"y":10.0254755,"p":1},{"arg_id":"A48_2","argument":"ç§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹æ±ºæ¸ˆã®å¤šæ§˜æ€§ã«ã¤ã„ã¦ã®ç†è§£ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"48","x":7.444315,"y":9.921515,"p":1},{"arg_id":"A49_0","argument":"70æ­³ä»¥ä¸Šã®äººã€…ã®ä¸­ã«ã¯ã€ç‰¹å®šã®æƒ…å ±ã‚„æŠ€è¡“ã«å¯¾ã™ã‚‹ç†è§£ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ã€‚","comment_id":"49","x":7.155922,"y":8.552295,"p":1},{"arg_id":"A49_1","argument":"è¿”å“ã—ãŸã„ã¨ã„ã†æ°—æŒã¡ã¯ã€å•†å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã«å¯¾ã™ã‚‹ä¸æº€ã‹ã‚‰ç”Ÿã˜ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"49","x":6.6847773,"y":10.299057,"p":1},{"arg_id":"A50_0","argument":"LINEã®ç™»éŒ²ã«ã¯ç›¸æ‰‹ã‹ã‚‰ã®èªè¨¼ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ã€‚","comment_id":"50","x":7.1509223,"y":10.929026,"p":1},{"arg_id":"A50_1","argument":"è¿·æƒ‘ãƒ¡ãƒ¼ãƒ«ã®è¨­å®šã«ã¤ã„ã¦ã®æƒ…å ±ãŒé‡è¦ã§ã‚ã‚‹ã€‚","comment_id":"50","x":7.5841413,"y":11.256763,"p":0.8963653735178357},{"arg_id":"A50_2","argument":"ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‹ã‚‰ãªã„ã¨ãã€ãƒ‘ã‚¹ã‚³ãƒ¼ãƒ‰ãŒå±Šã‹ãªã„ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"50","x":7.347666,"y":10.731888,"p":1},{"arg_id":"A51_0","argument":"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã—ãŸç›¸æ‰‹ãŒã™ã”ãå–œã‚“ã§è‡ªæ…¢ã—ã¦ããŸã€‚","comment_id":"51","x":5.422235,"y":9.738992,"p":0.8846517850013269},{"arg_id":"A51_2","argument":"å›°ã£ã¦ã„ã‚‹ã“ã¨ãŒè§£æ¶ˆã•ã‚Œã‚‹ã“ã¨ãŒå–œã°ã‚Œã‚‹ã€‚","comment_id":"51","x":5.5667024,"y":10.475273,"p":1},{"arg_id":"A51_3","argument":"è‡ªåˆ†ã«åˆã£ãŸæ–°ã—ã„ä½¿ã„æ–¹ãŒè¦‹ã¤ã‹ã‚‹ã“ã¨ãŒå–œã°ã‚Œã‚‹ã€‚","comment_id":"51","x":5.836836,"y":10.266381,"p":1},{"arg_id":"A51_4","argument":"å­ä¾›ã«ã€Œã“ã‚“ãªã“ã¨ã‚‚åˆ†ã‹ã‚‰ãªã„ã®ï¼Ÿã€ã¨è¨€ã‚ã‚Œãªããªã‚‹ã“ã¨ãŒå–œã°ã‚Œã‚‹ã€‚","comment_id":"51","x":5.9341764,"y":10.204947,"p":0},{"arg_id":"A52_4","argument":"ã‚·ãƒ‹ã‚¢ã®ä¸å®‰è¦ç´ ã‚’è§£æ±ºã™ã‚‹æ–¹æ³•ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"52","x":5.8023343,"y":8.921027,"p":1},{"arg_id":"A52_7","argument":"ãƒ‡ã‚¸ã‚¿ãƒ«æ™‚ä»£ã®å°‚é–€ç”¨èªãŒã‚·ãƒ‹ã‚¢ã®çµŒé¨“ã«åŸºã¥ã„ãŸè¨€è‘‰ã«ç½®ãæ›ãˆã‚‰ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"52","x":6.070187,"y":8.964514,"p":0.8893776666849208},{"arg_id":"A52_8","argument":"è‘—è€…ã¯20å¹´ä»¥ä¸Šã®çµŒé¨“ã‚’æŒã¡ã€ã‚·ãƒ‹ã‚¢ã«å‘ãåˆã£ã¦ããŸã€‚","comment_id":"52","x":6.260176,"y":8.642977,"p":0.646208931612379},{"arg_id":"A53_1","argument":"è‘—è€…ã®å¢—ç”°å…ˆç”Ÿã¯ã‚·ãƒ‹ã‚¢ä¸–ä»£ã¨æ¥ã—ã¦ã„ã‚‹ãŸã‚ã€å®Ÿè·µçš„ãªå†…å®¹ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"53","x":5.8852677,"y":8.694383,"p":1},{"arg_id":"A53_3","argument":"ã‚·ãƒ‹ã‚¢ä¸–ä»£ã®ãƒªã‚¢ãƒ«ãªä½“é¨“è«‡ãŒå…±æ„Ÿã‚„æ–°ã—ã„ç™ºè¦‹ã‚’ä¿ƒã™ã€‚","comment_id":"53","x":5.9881973,"y":8.781553,"p":1}]},{"cluster":"ä½¿ã„ã‚„ã™ã•ã¨ç†è§£ã—ã‚„ã™ã•","cluster_id":"0","takeaways":"å‚åŠ è€…ã®æ„è¦‹ã¯ã€æ›¸ç±ã®å†…å®¹ã«å¯¾ã™ã‚‹è©•ä¾¡ãŒåˆ†ã‹ã‚Œã¦ã„ã¾ã™ã€‚å¤šãã®äººãŒå†…å®¹ã®ã‚ã‹ã‚Šã‚„ã™ã•ã‚„è¦–è¦šçš„ãªè£œåŠ©ï¼ˆå†™çœŸã‚„å›³ï¼‰ã‚’è©•ä¾¡ã—ã€è¦ªåˆ‡ä¸å¯§ãªèª¬æ˜ãŒç†è§£ã‚’åŠ©ã‘ã‚‹ã¨æ„Ÿã˜ã¦ã„ã¾ã™ã€‚ä¸€æ–¹ã§ã€ç´¢å¼•ã®æ¬ å¦‚ã‚„å†…å®¹ã®ç‰©è¶³ã‚Šãªã•ã€é›£è§£ã•ã‚’æŒ‡æ‘˜ã™ã‚‹å£°ã‚‚ã‚ã‚Šã¾ã—ãŸã€‚\n\nå…¨ä½“ã¨ã—ã¦ã€èª­ã¿ã‚„ã™ã•ã‚„æ§‹æˆã®è‰¯ã•ãŒå¼·èª¿ã•ã‚Œã‚‹ä¸€æ–¹ã§ã€ã•ã‚‰ãªã‚‹å……å®Ÿã‚’æ±‚ã‚ã‚‹æ„è¦‹ã‚‚è¦‹å—ã‘ã‚‰ã‚Œã¾ã™ã€‚æ—¢ã«çŸ¥è­˜ãŒã‚ã‚‹èª­è€…ã«ã¯ç‰©è¶³ã‚Šãªã•ã‚’æ„Ÿã˜ã‚‹éƒ¨åˆ†ãŒã‚ã‚Šã€å†…å®¹ã®æ·±ã•ã«å¯¾ã™ã‚‹æœŸå¾…ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚","arguments":[{"arg_id":"A2_0","argument":"ä½¿ã„é“ã‚„å›°ã‚Šã”ã¨ãŒç´¢å¼•ã«ãªã£ã¦ã„ã‚‹ç‚¹ãŒè‰¯ã„ã€‚","comment_id":"2","x":5.1182237,"y":11.192371,"p":0},{"arg_id":"A3_0","argument":"ã“ã®æœ¬ã¯ã‚„ã•ã—ãã€ä¸å¯§ã«åˆ†ã‹ã‚Šã‚„ã™ãæ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"3","x":3.5789306,"y":10.961764,"p":1},{"arg_id":"A3_1","argument":"ã“ã®æœ¬ã¯å„ªã‚ŒãŸå†…å®¹ã‚’æŒã£ã¦ã„ã‚‹ã€‚","comment_id":"3","x":3.409111,"y":10.171141,"p":0.8168539383493483},{"arg_id":"A6_0","argument":"ã“ã®æœ¬ã¯åŸºæœ¬ã‹ã‚‰è§£ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ã„ã‚‹ã€‚","comment_id":"6","x":3.6522377,"y":11.113901,"p":0.5886459506242002},{"arg_id":"A7_0","argument":"ç§ã¯ã“ã®æœ¬ã‚’ä½¿ã„ã“ãªã›ã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚","comment_id":"7","x":3.3956785,"y":9.270017,"p":0.9332438495886864},{"arg_id":"A9_1","argument":"ç´¢å¼•ãŒç„¡ã„ã®ã§ã€ã“ã®æœ¬ã¯ä½¿ã„ã«ãã„ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"9","x":4.0068665,"y":10.2924595,"p":0.7786359301693038},{"arg_id":"A10_0","argument":"ã“ã®æœ¬ã¯éå¸¸ã«è‰¯ã‹ã£ãŸã§ã™ã€‚","comment_id":"10","x":3.2818668,"y":9.979671,"p":1},{"arg_id":"A10_1","argument":"å†…å®¹ãŒã‚ã‹ã‚Šã‚„ã™ãã€èˆˆå‘³ã‚’å¼•ãã¾ã—ãŸã€‚","comment_id":"10","x":4.272762,"y":11.781682,"p":1},{"arg_id":"A10_2","argument":"èª­ã¿ã‚„ã™ã„æ–‡ç« ã§ã€ã™ãã«å¼•ãè¾¼ã¾ã‚Œã¾ã—ãŸã€‚","comment_id":"10","x":4.1594057,"y":12.003611,"p":0.945739649638884},{"arg_id":"A10_3","argument":"å…¨ä½“çš„ã«æº€è¶³ã®ã„ãä½“é¨“ã§ã—ãŸã€‚","comment_id":"10","x":5.2105627,"y":9.869944,"p":1},{"arg_id":"A11_0","argument":"æ˜“ã—ã„æ–‡ç« ã¯ç†è§£ã—ã‚„ã™ãã€å¾©ç¿’ã«æœ€é©ã§ã‚ã‚‹ã€‚","comment_id":"11","x":4.371473,"y":11.960884,"p":1},{"arg_id":"A11_1","argument":"å†™çœŸãŒã‚ã‚‹ã“ã¨ã§ã€è¦–è¦šçš„ã«æƒ…å ±ã‚’è£œå®Œã§ãã‚‹ã€‚","comment_id":"11","x":5.4610496,"y":11.763461,"p":1},{"arg_id":"A11_2","argument":"ã“ã®æœ¬ã¯å¾©ç¿’ã‚’åŠ¹æœçš„ã«è¡Œã†ãŸã‚ã®è‰¯ã„ãƒªã‚½ãƒ¼ã‚¹ã§ã‚ã‚‹ã€‚","comment_id":"11","x":3.7650893,"y":10.6965065,"p":1},{"arg_id":"A15_2","argument":"åˆå¿ƒè€…ã«ã¯ã“ã®æœ¬ã®å†…å®¹ãŒç†è§£ã§ããªã„å¯èƒ½æ€§ãŒé«˜ã„ã€‚","comment_id":"15","x":5.5619636,"y":10.73251,"p":0},{"arg_id":"A16_0","argument":"ã“ã®æœ¬ã¯å®¶æ—ã«å†…å®¹ã‚’æ•™ãˆã‚‹ã®ã«å½¹ç«‹ã¤ã€‚","comment_id":"16","x":3.8704677,"y":10.641121,"p":0.660816937319749},{"arg_id":"A16_1","argument":"ç§ã¯ã“ã®æœ¬ãŒå‚è€ƒã«ãªã‚‹ã¨æ„Ÿã˜ãŸã€‚","comment_id":"16","x":3.384559,"y":9.424571,"p":0.8497331391134264},{"arg_id":"A18_1","argument":"é …ç›®ã”ã¨ã«æ•´ç†ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€å¿…è¦ãªæƒ…å ±ã‚’æ¢ã—ã‚„ã™ã„ã€‚","comment_id":"18","x":5.2672358,"y":12.201703,"p":0},{"arg_id":"A18_2","argument":"å›°ã‚Šäº‹ãŒåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç†è§£ã—ã‚„ã™ã„ã€‚","comment_id":"18","x":4.8770957,"y":11.534462,"p":0},{"arg_id":"A19_0","argument":"ã“ã®æœ¬ã¯å„ªã—ãã¦åˆ†ã‹ã‚Šã‚„ã™ã„å†…å®¹ã§ã€åŠ©ã‘ã‚‰ã‚ŒãŸã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"19","x":3.3984323,"y":10.680363,"p":0.5506872735630878},{"arg_id":"A20_0","argument":"ç§ã¯ã“ã®æœ¬ã®å†…å®¹ã®åŠåˆ†ã»ã©ã‚’ã™ã§ã«çŸ¥ã£ã¦ã„ã¾ã—ãŸãŒã€åˆã‚ã¦çŸ¥ã£ãŸã“ã¨ã‚‚ã‚ã‚Šã€å½¹ã«ç«‹ã¡ã¾ã—ãŸã€‚","comment_id":"20","x":3.612908,"y":9.096841,"p":1},{"arg_id":"A21_0","argument":"ã“ã®æœ¬ã¯ã‚¿ã‚¤ãƒˆãƒ«ãŒç¤ºã™ã»ã©æ˜“ã—ãã¯ãªã„ã€‚","comment_id":"21","x":3.7003324,"y":10.232794,"p":1},{"arg_id":"A21_1","argument":"æœ¬æ›¸ã«ã¯å¤šãã®æœªè§£æ±ºã®å•é¡ŒãŒæ®‹ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"21","x":4.4852138,"y":10.704371,"p":1},{"arg_id":"A22_0","argument":"æ–‡å­—ãŒå¤§ããã¦èª­ã¿ã‚„ã™ã„ã€‚","comment_id":"22","x":3.7077713,"y":12.077973,"p":0.8088483226690135},{"arg_id":"A22_1","argument":"è¦ªåˆ‡ä¸å¯§ãªå†…å®¹ã§ã‚ã‚‹ã€‚","comment_id":"22","x":4.96805,"y":10.3425665,"p":0},{"arg_id":"A22_2","argument":"ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜ãŒã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"22","x":4.951569,"y":11.925593,"p":1},{"arg_id":"A23_1","argument":"ã“ã®æœ¬ã¯è–„ã£ãºã‚‰ã§å†…å®¹ãŒãªã„ã€‚","comment_id":"23","x":3.7258167,"y":10.136287,"p":0.9423460659984736},{"arg_id":"A23_2","argument":"1,650å††ï¼ˆç¨è¾¼ã¿ï¼‰ã¯é©šãã¹ãä¾¡æ ¼ã ã€‚","comment_id":"23","x":5.3506365,"y":10.118323,"p":0.9949936926661648},{"arg_id":"A23_4","argument":"æœ¬ã®å†…å®¹ã¯èŒ¶é£²ã¿è©±ãŒåŠåˆ†ã‚’å ã‚ã¦ã„ã‚‹ã€‚","comment_id":"23","x":3.9289937,"y":9.799028,"p":0},{"arg_id":"A24_0","argument":"ã“ã®æœ¬ã¯éå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ã„å†…å®¹ã§ã€åˆå¿ƒè€…ã§ã‚‚ç†è§£ã—ã‚„ã™ã„ã€‚","comment_id":"24","x":3.3157768,"y":11.035838,"p":0.567797270608892},{"arg_id":"A24_1","argument":"èª¬æ˜ãŒæ˜ç¢ºã§ã€é›£ã—ã„æ¦‚å¿µã‚‚ç°¡å˜ã«ç†è§£ã§ãã‚‹ã‚ˆã†ã«å·¥å¤«ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"24","x":4.881216,"y":11.916125,"p":1},{"arg_id":"A24_2","argument":"å›³ã‚„ä¾‹ãŒè±Šå¯Œã§ã€è¦–è¦šçš„ã«ç†è§£ã‚’åŠ©ã‘ã¦ãã‚Œã‚‹ã€‚","comment_id":"24","x":5.2150636,"y":11.798983,"p":1},{"arg_id":"A24_3","argument":"æ–‡ç« ãŒã‚·ãƒ³ãƒ—ãƒ«ã§ã€å°‚é–€ç”¨èªãŒå°‘ãªã„ãŸã‚ã€ã‚¹ãƒ ãƒ¼ã‚ºã«èª­ã¿é€²ã‚ã‚‰ã‚Œã‚‹ã€‚","comment_id":"24","x":4.221423,"y":12.232971,"p":0.7621669382471801},{"arg_id":"A24_4","argument":"å…¨ä½“çš„ã«æ§‹æˆãŒè‰¯ãã€æ®µéšçš„ã«å­¦ã¹ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã€‚","comment_id":"24","x":5.045677,"y":11.654009,"p":1},{"arg_id":"A26_1","argument":"ã“ã®æœ¬ã¯ã€ç§ã«ã¯ã™ã§ã«çŸ¥è­˜ãŒã‚ã£ãŸãŸã‚ã€å°‘ã—ç‰©è¶³ã‚Šãªã•ã‚’æ„Ÿã˜ãŸã€‚","comment_id":"26","x":3.7537713,"y":9.149649,"p":1},{"arg_id":"A26_2","argument":"æœ¬ã®å†…å®¹ã¯æ°—æ¥½ã«èª­ã‚ã‚‹ã‚‚ã®ã§ã€åŠ›ãŒã¤ã‘ã°æ¨æ¸¬ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚","comment_id":"26","x":3.9917173,"y":11.323606,"p":0},{"arg_id":"A27_0","argument":"ã“ã®æœ¬ã¯å†…å®¹ãŒé›£è§£ã§ç†è§£ã—ã¥ã‚‰ã„ã€‚","comment_id":"27","x":3.938324,"y":10.282651,"p":1},{"arg_id":"A30_0","argument":"ã“ã®æœ¬ã¯ã€ãªã‚“ã¨ãªãã®æ„Ÿè¦šã‚’ç¢ºå®Ÿãªã‚‚ã®ã«ã—ã¦ãã‚Œã‚‹ã€‚","comment_id":"30","x":3.1394448,"y":10.695461,"p":1},{"arg_id":"A30_1","argument":"å†…å®¹ãŒã‚ã‹ã‚Šã‚„ã™ãã€ç†è§£ã—ã‚„ã™ã„ã€‚","comment_id":"30","x":4.4684043,"y":12.127822,"p":0.7621669382471801},{"arg_id":"A30_2","argument":"èª­ã‚“ã§ã„ã‚‹ã¨ã€è‡ªä¿¡ã‚’æŒã£ã¦ç‰©äº‹ã‚’åˆ¤æ–­ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚","comment_id":"30","x":4.42142,"y":11.190393,"p":0.5470870338480566},{"arg_id":"A32_0","argument":"ã“ã®æœ¬ã¯èª­ã¿ã‚„ã™ã„ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"32","x":3.048864,"y":11.18352,"p":0},{"arg_id":"A33_3","argument":"æœ¬æ›¸ã®æ‰‹é †ã«å¾“ã£ã¦ä¸€ã¤ä¸€ã¤ç¢ºèªã—ã¦ã„ã‘ã°ã€æ“ä½œãŒã‚¯ãƒªã‚¢ã§ãã‚‹ã€‚","comment_id":"33","x":5.926574,"y":12.033929,"p":1},{"arg_id":"A33_4","argument":"å…¨ã¦ã®é“ã¯ãƒ›ãƒ¼ãƒ ã«é€šã˜ã‚‹ã®ã§ã€ãƒ›ãƒ¼ãƒ ç”»é¢ã«æˆ»ã£ã¦æ“ä½œã‚’ç¢ºèªã—ã¦ã„ã‚‹ã€‚","comment_id":"33","x":6.0858326,"y":12.097393,"p":1},{"arg_id":"A34_0","argument":"ç¢ºèªã®ãŸã‚ã«ä¾¿åˆ©ãªãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹ã¨ã€çŸ¥è­˜ã‚’å†ç¢ºèªã§ãã‚‹ã€‚","comment_id":"34","x":5.8707204,"y":11.857971,"p":1},{"arg_id":"A34_1","argument":"çŸ¥ã£ã¦ã„ã‚‹ã“ã¨ã§ã‚‚ã€ç¢ºèªã™ã‚‹ã“ã¨ã§ç†è§£ãŒæ·±ã¾ã‚‹ã€‚","comment_id":"34","x":5.8061366,"y":11.437292,"p":0},{"arg_id":"A35_2","argument":"ã“ã®æœ¬ã¯æœ¬å½“ã«å¿…è¦ãªæƒ…å ±ã‚’åˆ†ã‹ã‚Šã‚„ã™ãæ•™ãˆã¦ãã‚Œã‚‹ã€‚","comment_id":"35","x":3.6797848,"y":10.820952,"p":1},{"arg_id":"A35_3","argument":"æ–‡å­—ãŒå¤§ããã¦èª­ã¿ã‚„ã™ã„ã€‚","comment_id":"35","x":3.6590505,"y":12.143131,"p":1},{"arg_id":"A36_0","argument":"ã“ã®æœ¬ã¯ç§ã«ã¨ã£ã¦ã¾ã£ãŸãå½¹ã«ç«‹ãŸãªã‹ã£ãŸã€‚","comment_id":"36","x":3.5860193,"y":9.558101,"p":0.7980656623294309},{"arg_id":"A36_1","argument":"å†…å®¹ã¯ç§ãŒã™ã§ã«çŸ¥ã£ã¦ã„ã‚‹ã“ã¨ã°ã‹ã‚Šã ã£ãŸã€‚","comment_id":"36","x":3.8144135,"y":9.021787,"p":0.9551073949474248},{"arg_id":"A37_1","argument":"å›³æ›¸é¤¨ã§ä¸€åº¦ãã®æœ¬ã‚’èª­ã‚“ã ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚","comment_id":"37","x":3.3287892,"y":9.173241,"p":0.79775218731579},{"arg_id":"A37_2","argument":"ãã®æœ¬ã¯æ–‡å­—ãŒå¤§ããã€èª¬æ˜ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã§ã™ã€‚","comment_id":"37","x":3.571602,"y":11.415066,"p":0},{"arg_id":"A39_0","argument":"å†…å®¹ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã¨ã€èª­è€…ã¯ç†è§£ã—ã‚„ã™ããªã‚‹ã€‚","comment_id":"39","x":4.1663485,"y":11.937088,"p":1},{"arg_id":"A44_0","argument":"ç§ã¯ã“ã®æœ¬ãŒåˆæ­©çš„ãªçŸ¥è­˜ã‚’å¾—ã‚‹ã®ã«æœ€é©ã ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"44","x":3.613377,"y":9.302217,"p":0.9905561503694308},{"arg_id":"A44_2","argument":"ã“ã®æœ¬ã®å†…å®¹ã¯ã€ç§ã«ã¨ã£ã¦ã¡ã‚‡ã†ã©è‰¯ã„ãƒ¬ãƒ™ãƒ«ã ã¨æ€ã†ã€‚","comment_id":"44","x":3.2235348,"y":9.667761,"p":1},{"arg_id":"A44_3","argument":"ãã‚Œä»¥ä¸Šã®çŸ¥è­˜ã¯ã€è‡ªåˆ†ã§å­¦ã‚“ã§ã„ã‘ã°è‰¯ã„ã¨è€ƒãˆã‚‹ã€‚","comment_id":"44","x":4.154601,"y":9.094833,"p":0},{"arg_id":"A46_1","argument":"ã‚‚ã†å°‘ã—å†…å®¹ãŒå……å®Ÿã—ã¦ã„ã¦ã‚‚è‰¯ã‹ã£ãŸã¨æ€ã†ã€‚","comment_id":"46","x":4.702581,"y":9.884985,"p":1},{"arg_id":"A46_2","argument":"å­—ãŒå¤§ããã€ã‚¹ãƒšãƒ¼ã‚¹ãŒåºƒã„ãŸã‚ã€èª­ã¿ã‚„ã™ã•ã¯ã‚ã‚‹ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"46","x":3.666619,"y":12.173501,"p":1},{"arg_id":"A46_3","argument":"å†…å®¹çš„ã«ã¯ã‚‚ã†ä¸€æ­©é€²ã‚“ã§ã»ã—ã„ã¨æ€ã†ã€‚","comment_id":"46","x":4.6465864,"y":10.082296,"p":1},{"arg_id":"A46_6","argument":"ç¶šç·¨ã‚„ä¸Šç´šç·¨ãŒã‚ã‚Œã°è‰¯ã„ã¨è€ƒãˆã‚‹ã€‚","comment_id":"46","x":4.6834927,"y":9.8542185,"p":1},{"arg_id":"A47_1","argument":"æœ¬ã®é¡Œåã«æƒ¹ã‹ã‚Œã¦è³¼å…¥ã—ãŸãŒã€ä¸­èº«ã‚’è©¦ã—è¦‹ã§ããªã‹ã£ãŸã®ãŒæ®‹å¿µã§ã™ã€‚","comment_id":"47","x":4.233997,"y":10.069929,"p":0},{"arg_id":"A51_6","argument":"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã—ã¦å–œã‚“ã§ã‚‚ã‚‰ãˆãŸã“ã¨ãŒå¬‰ã—ã‹ã£ãŸã®ã§ã€è©•ä¾¡ã¯â˜…5ã§ã‚ã‚‹ã€‚","comment_id":"51","x":5.1535892,"y":9.914183,"p":1},{"arg_id":"A52_1","argument":"è‘—è€…ã®æ¸©ã‹ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ„Ÿã˜ã‚‰ã‚Œã‚‹è‡ªå·±å•“ç™ºæ›¸ã§ã‚ã‚‹ã€‚","comment_id":"52","x":3.1261234,"y":10.436752,"p":1},{"arg_id":"A52_3","argument":"ãµã‚“ã ã‚“ãªæ´»ç”¨äº‹ä¾‹ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"52","x":5.3840427,"y":11.37526,"p":1},{"arg_id":"A52_5","argument":"å„ç¨®è¨­å®šã‚„æ“ä½œæ–¹æ³•ãŒé …ç›®æ¯ã«å›³è§£ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"52","x":5.4621377,"y":12.184752,"p":0},{"arg_id":"A52_6","argument":"èª­ã¿ã‚„ã™ã„å¤§ããªæ–‡å­—ã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ç·¨é›†ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"52","x":3.6193311,"y":12.243086,"p":0.7822074911497644},{"arg_id":"A52_9","argument":"ã“ã®æœ¬ã¯è‘—è€…ã®ç§€é€¸ã•ã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚·ãƒªãƒ¼ã‚ºã®æœ€æ–°ä½œã§ã‚ã‚‹ã€‚","comment_id":"52","x":3.087457,"y":10.275312,"p":0},{"arg_id":"A54_0","argument":"ã“ã®æœ¬ã¯ã€è§£æ±ºã—ãŸã„å›°ã‚Šäº‹ãŒå¤šãæ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"54","x":4.6434326,"y":10.770651,"p":1},{"arg_id":"A54_1","argument":"è¡Œé–“éš”ãŒåºƒãã€èª­ã¿ã‚„ã™ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã«ãªã£ã¦ã„ã‚‹ã€‚","comment_id":"54","x":3.8416157,"y":12.1005745,"p":0.4488007055320658},{"arg_id":"A54_2","argument":"ã‚¤ãƒ©ã‚¹ãƒˆãŒè±Šå¯Œã§ã€å†…å®¹ãŒã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ã•ã‚Œã¦ã„ã‚‹ã€‚","comment_id":"54","x":4.841203,"y":11.979522,"p":1},{"arg_id":"A54_3","argument":"ãƒšãƒ¼ã‚¸ã‚’ã‚ãã‚‹ã“ã¨ã§ã€èª­ã¿ãŸã„æƒ…å ±ã‚’ç°¡å˜ã«æ¢ã›ã‚‹ã€‚","comment_id":"54","x":4.9994345,"y":12.393926,"p":0},{"arg_id":"A54_4","argument":"å›°ã‚Šäº‹ã‚’ç†è§£ã™ã‚‹æ¥½ã—ã•ã‚’æä¾›ã—ã¦ã„ã‚‹æœ¬ã§ã‚ã‚‹ã€‚","comment_id":"54","x":4.726085,"y":10.801665,"p":1}]},{"cluster":"ã‚¹ãƒãƒ›æ“ä½œã®æ©Ÿç¨®åˆ¥æƒ…å ±ä¸è¶³","cluster_id":"1","takeaways":"å‚åŠ è€…ã¯ã€ã‚¹ãƒãƒ›ã®æ“ä½œã«é–¢ã™ã‚‹æœ¬ãŒé«˜é½¢è€…ã«ã¨ã£ã¦å½¹ç«‹ã¤ã¨è©•ä¾¡ã—ã¦ã„ã¾ã™ãŒã€å†…å®¹ãŒiPhoneã«åã£ã¦ã„ã‚‹ã¨æ„Ÿã˜ã¦ã„ã¾ã™ã€‚ç‰¹ã«ã€Androidãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ã¯æƒ…å ±ãŒä¸è¶³ã—ã¦ãŠã‚Šã€æ“ä½œæ–¹æ³•ãŒç•°ãªã‚‹ã“ã¨ãŒå¤šã„ã¨æŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nã¾ãŸã€åˆå¿ƒè€…å‘ã‘ã®ã‚ã‹ã‚Šã‚„ã™ã„èª¬æ˜ãŒå¥½è©•ã§ã™ãŒã€ç‰¹å®šã®çŠ¶æ³ã¸ã®å¯¾å‡¦æ³•ãŒä¸­å¿ƒã§ã€åŸºæœ¬çš„ãªæ“ä½œæ–¹æ³•ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨ã®æ„è¦‹ã‚‚ã‚ã‚Šã¾ã™ã€‚å…¨ä½“ã¨ã—ã¦ã€ã“ã®æœ¬ã¯é«˜é½¢è€…ãŒã‚¹ãƒãƒ›ã‚’æ´»ç”¨ã™ã‚‹ãŸã‚ã®å®Ÿç”¨çš„ãªã‚¬ã‚¤ãƒ‰ã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã‚ˆã‚Šå¤šæ§˜ãªæ©Ÿç¨®ã«å¯¾å¿œã—ãŸå†…å®¹ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚","arguments":[{"arg_id":"A4_0","argument":"ç§ã¯ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå‚è€ƒã«ãªã‚‹ã‹ã©ã†ã‹ã¯å¾®å¦™ã ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"4","x":9.763085,"y":10.668023,"p":1},{"arg_id":"A12_0","argument":"ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã‚¢ã‚¤ãƒ›ãƒ³ã®ä¸¡æ–¹ã®ç‰¹å¾´ã‚’èª¬æ˜ã—ã¦ã„ã‚‹ã“ã¨ãŒè‰¯ã„ã€‚","comment_id":"12","x":9.834531,"y":10.692378,"p":1},{"arg_id":"A13_0","argument":"iPhoneã«ç‰¹åŒ–ã—ãŸå†…å®¹ãŒå¤šãã€Androidãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ä¸æº€ãŒæ®‹ã‚‹ã€‚","comment_id":"13","x":9.761358,"y":10.414989,"p":0.4072930194676928},{"arg_id":"A13_1","argument":"ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒiPhoneã®æ©Ÿèƒ½ã‚„åˆ©ç‚¹ã«åã£ã¦ã„ã‚‹ãŸã‚ã€ä»–ã®ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯å‚è€ƒã«ãªã‚Šã«ãã„ã€‚","comment_id":"13","x":9.563364,"y":10.46595,"p":0.2924348124898321},{"arg_id":"A17_0","argument":"ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã‚¢ã‚¤ãƒ•ã‚©ãƒ³ã®è§£èª¬ãŒåˆ¥ã€…ã«ã•ã‚Œã¦ã„ã‚‹ãŒã€æ©Ÿç¨®ã«ã‚ˆã£ã¦ç”»é¢ãŒå¤§ããç•°ãªã‚‹ãŸã‚ã€ãŠå¹´å¯„ã‚Šã«ã¯ç†è§£ãŒé›£ã—ã„ã€‚","comment_id":"17","x":9.877472,"y":10.501099,"p":0.5747535826235006},{"arg_id":"A23_5","argument":"ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã‚¢ã‚¤ãƒ•ã‚©ãƒ³ç‰ˆãŒã‚ã‚Šã€å®Ÿè³ªçš„ã«å†…å®¹ãŒ1/4ã«ãªã£ã¦ã„ã‚‹ã€‚","comment_id":"23","x":9.808647,"y":10.613427,"p":1},{"arg_id":"A25_0","argument":"ã“ã®æœ¬ã¯ã€ã‚¹ãƒãƒ›ã«æ…£ã‚ŒãŸäººã«ã¨ã£ã¦ã¯å½“ãŸã‚Šå‰ã®å†…å®¹ãŒå¤šã„ã€‚","comment_id":"25","x":8.884115,"y":10.281534,"p":0.9442572432618448},{"arg_id":"A25_1","argument":"Androidã‚¹ãƒãƒ›ã§ã¯ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„ãƒ¡ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã£ã¦ç”»é¢ãŒç•°ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚","comment_id":"25","x":9.712941,"y":10.21128,"p":0.281314415386357},{"arg_id":"A25_3","argument":"ä½™åˆ†ãªæ–‡ç« ãŒå¤šãã€Androidæ©Ÿç¨®ã«å¯¾ã™ã‚‹æ¯”è¼ƒæ¤œè¨ãŒä¸ååˆ†ã§ã‚ã‚‹ã€‚","comment_id":"25","x":9.42508,"y":10.77225,"p":0.2348973900130701},{"arg_id":"A26_0","argument":"é«˜é½¢è€…ãŒã‚¹ãƒãƒ›ã‚’ä½¿ã„ã“ãªã™ã“ã¨ã¯éå¸¸ã«ä¾¿åˆ©ã§ã€é ­ã®é‹å‹•ã«ã‚‚ãªã‚‹ã€‚","comment_id":"26","x":8.782173,"y":9.385458,"p":1},{"arg_id":"A26_3","argument":"å…¨ä½“çš„ã«ã€ã‚¹ãƒãƒ›ã®ä½¿ã„æ–¹ã‚’å­¦ã¶ã®ã«é©ã—ãŸæœ¬ã ã¨æ€ã†ã€‚","comment_id":"26","x":8.937648,"y":9.958264,"p":1},{"arg_id":"A28_0","argument":"80æ­³ã‚’éããŸçŸ¥ã‚Šåˆã„ã®ãŸã‚ã«ã‚¹ãƒãƒ›ã‚’è³¼å…¥ã—ãŸãŒã€æ“ä½œãŒé›£ã—ã„ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"28","x":8.431045,"y":8.896814,"p":1},{"arg_id":"A31_0","argument":"ã‚¹ãƒãƒ›åˆå¿ƒè€…ã«ã¨ã£ã¦ã€èª¬æ˜ãŒã‚ã‹ã‚Šã‚„ã™ã„ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"31","x":8.709556,"y":10.094169,"p":0.8854239851382834},{"arg_id":"A31_3","argument":"å°‘ã—ãšã¤è¦šãˆã¦ã‚¹ãƒãƒ›ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã„ããŸã„ã¨è€ƒãˆã¦ã„ã‚‹ã€‚","comment_id":"31","x":8.91966,"y":9.605947,"p":0},{"arg_id":"A33_0","argument":"ç§ã¯78æ­³ã§ã€åˆã‚ã¦æœ¬æ›¸ã‚’ä½¿ã£ã¦ã‚¹ãƒãƒ›ã®æ“ä½œã‚’å­¦ã‚“ã§ã„ã‚‹ã€‚","comment_id":"33","x":8.508866,"y":8.9204,"p":0.5816073279148519},{"arg_id":"A33_1","argument":"æœ¬æ›¸ã¯ä¸–ç•Œä¸€ç°¡å˜ãªã‚¹ãƒãƒ›ã®æ“ä½œã‚’æ•™ãˆã¦ãã‚Œã‚‹ã€‚","comment_id":"33","x":8.672992,"y":9.9199705,"p":1},{"arg_id":"A33_2","argument":"ç§ã¯120ãƒšãƒ¼ã‚¸ã§ã€70æ­³ã‹ã‚‰ã®ã‚¹ãƒãƒ›ã®ã¤ã¾ã¥ãåŸå› ãƒ™ã‚¹ãƒˆ3ã‚’ç¢ºèªã—ã¦ã„ã‚‹ã€‚","comment_id":"33","x":8.390336,"y":8.841068,"p":1},{"arg_id":"A35_1","argument":"ç”Ÿæ´»ã«å¿…è¦ãªæ©Ÿèƒ½ã¯ãã‚Œã»ã©å¤šãã¯ãªã„ã¨æ„Ÿã˜ã‚‹ã€‚","comment_id":"35","x":8.314469,"y":10.829595,"p":0.8298997897978274},{"arg_id":"A37_3","argument":"æ¯ã¯æ‰‹å…ƒã«ç½®ã„ã¦ãŠããŸã„ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚","comment_id":"37","x":8.781524,"y":8.680019,"p":0},{"arg_id":"A40_0","argument":"ã‚¹ãƒãƒ›ã®å¤šæ§˜ãªä½¿ã„æ–¹ã«ã¤ã„ã¦ã®æƒ…å ±ãŒéå¸¸ã«å‚è€ƒã«ãªã‚‹ã€‚","comment_id":"40","x":9.19917,"y":10.151245,"p":0},{"arg_id":"A40_1","argument":"ã“ã®æœ¬ã¯ã‚¹ãƒãƒ›ã‚’æ´»ç”¨ã™ã‚‹ãŸã‚ã®å®Ÿç”¨çš„ãªã‚¬ã‚¤ãƒ‰ã§ã‚ã‚‹ã€‚","comment_id":"40","x":8.804781,"y":9.877519,"p":1},{"arg_id":"A42_0","argument":"ç§ã¯70ä»£ã®çˆ¶è¦ªã«ã‚¹ãƒãƒ›ã‚’ä½¿ã‚ã›ã‚‹ãŸã‚ã«ã“ã®æœ¬ã‚’è³¼å…¥ã—ã¾ã—ãŸã€‚","comment_id":"42","x":8.427108,"y":8.834368,"p":1},{"arg_id":"A42_1","argument":"çˆ¶è¦ªã¨ä¸€ç·’ã«æš®ã‚‰ã—ã¦ã„ãªã„ã®ã§ã€ä½¿ã„æ–¹ã‚’æ•™ãˆã‚‹ã®ãŒé›£ã—ã„ã§ã™ã€‚","comment_id":"42","x":8.95115,"y":8.8106,"p":0},{"arg_id":"A42_2","argument":"çˆ¶è¦ªã¯å…ƒã€…ãƒ‘ã‚½ã‚³ãƒ³ãŒå¥½ããªã®ã§ã€ç‹¬åŠ›ã§ã‚¹ãƒãƒ›ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ãã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚","comment_id":"42","x":8.902143,"y":9.138886,"p":0},{"arg_id":"A46_4","argument":"ã‚¢ãƒ—ãƒªã®ã¾ã¨ã‚æ–¹ã¯æ›¸ã„ã¦ã‚ã‚‹ãŒã€ä¸¦ã¹æ›¿ãˆã®ä»•æ–¹ãŒæ›¸ã„ã¦ãªã„ã®ã¯æ®‹å¿µã ã€‚","comment_id":"46","x":8.942228,"y":10.825217,"p":0},{"arg_id":"A47_0","argument":"ç§ã¯ã€ã‚¹ãƒãƒ›ã®æ“ä½œãŒåˆ†ã‹ã‚‰ãªã„æ¯ã®ãŸã‚ã«ã“ã®æœ¬ã‚’è³¼å…¥ã—ã¾ã—ãŸã€‚","comment_id":"47","x":8.640092,"y":9.093135,"p":0.2594958039729968},{"arg_id":"A47_2","argument":"ç§ã¯åŸºæœ¬çš„ãªã‚¹ãƒãƒ›ã®æ“ä½œæ–¹æ³•ãŒè¼‰ã£ã¦ã„ã‚‹ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã¯ç‰¹å®šã®çŠ¶æ³ã«å¯¾ã™ã‚‹å¯¾å‡¦æ³•ãŒä¸­å¿ƒã§ã—ãŸã€‚","comment_id":"47","x":9.056295,"y":10.2387295,"p":1},{"arg_id":"A47_3","argument":"ã‚ã‚‹ç¨‹åº¦ã®ã‚¹ãƒãƒ›çŸ¥è­˜ãŒã‚ã‚‹äººã«ã¯ç†è§£ã§ãã‚‹å†…å®¹ã ã¨æ€ã„ã¾ã™ãŒã€åˆå¿ƒè€…ã«ã¯é›£ã—ã„ã¨æ„Ÿã˜ã¾ã™ã€‚","comment_id":"47","x":8.402456,"y":10.293341,"p":0},{"arg_id":"A51_1","argument":"ã‚¹ãƒãƒ›ãŒã†ã¾ãä½¿ãˆãªã„äººã«ã¨ã£ã¦ã€ç´°ã‹ã„æ©Ÿèƒ½ã‚„ä»•çµ„ã¿ã¯é‡è¦ã§ã¯ãªã„ã€‚","comment_id":"51","x":9.126167,"y":10.496492,"p":0},{"arg_id":"A51_5","argument":"ã“ã®æœ¬ã¯ã€ŒæŠ€è¡“æ›¸ã€ã§ã¯ãªãã€Œç”Ÿæ´»ã«ã€ã†ã¾ãä½¿ãˆã‚‹å°æŠ€é›†ã€ã®ã‚ˆã†ãªå†…å®¹ã§ã‚ã‚‹ã€‚","comment_id":"51","x":8.482986,"y":10.628066,"p":0.8089905482075269},{"arg_id":"A52_0","argument":"æœ¬æ›¸ã¯é«˜é½¢è€…ãŒã‚¹ãƒãƒ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã‚·ãƒ‹ã‚¢ãƒ©ã‚¤ãƒ•ã«æ´»åŠ›ã¨æ¥½ã—ã¿ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ä¼ãˆã‚‹ã€‚","comment_id":"52","x":8.703363,"y":9.351507,"p":1},{"arg_id":"A52_2","argument":"å†…å®¹ã¯ã‚¹ãƒãƒ›ã‚’ç”Ÿæ´»ã®ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã™ã‚‹ãƒã‚¤ãƒ³ãƒ‰ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚","comment_id":"52","x":9.21862,"y":9.795726,"p":1},{"arg_id":"A53_0","argument":"ã‚¹ãƒãƒ›ã®ä½¿ã„æ–¹ã¯ã‚·ãƒ‹ã‚¢ä¸–ä»£ã«ç‰¹æœ‰ã®è¦–ç‚¹ãŒå¿…è¦ã§ã‚ã‚‹ã€‚","comment_id":"53","x":9.24793,"y":9.600071,"p":0},{"arg_id":"A53_2","argument":"å…·ä½“çš„ãªã‚¹ãƒãƒ›ã®ä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„äººã«ã¨ã£ã¦ã€ã“ã®æœ¬ã¯å½¹ç«‹ã¤æƒ…å ±ã‚’æä¾›ã™ã‚‹ã€‚","comment_id":"53","x":8.945184,"y":10.34976,"p":1},{"arg_id":"A53_4","argument":"ã‚¹ãƒãƒ›ã®ä½¿ã„æ–¹ã‚’å­¦ã¶ã“ã¨ã§ã€æ—¥å¸¸ç”Ÿæ´»ãŒä¾¿åˆ©ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚","comment_id":"53","x":9.105534,"y":9.832881,"p":1}]}],"comments":{"1":{"comment":"ã‚„ã¯ã‚Šã€é«˜é½¢è€…ã«ã¯é›£èªå¥ãŒå¤šãã¦å›°ã£ãŸã€"},"2":{"comment":"ä½¿ã„é“ã€å›°ã‚Šã”ã¨ãŒç´¢å¼•ã«ãªã£ã¦ã„ã‚‹ç‚¹ãŒè‰¯ã„ã€‚"},"3":{"comment":"ã‚„ã•ã—ãã€ã‹ã¤ä¸å¯§ã«åˆ†ã‹ã‚Šã‚„ã™ãæ›¸ã„ã¦ã‚ã‚Šå„ªã‚Œã‚‚ã®ã§ã™ã€‚"},"4":{"comment":"ä½¿ç”¨ã—ã¦ã„ã‚‹ã®ãŒã€ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ãªã®ã§ã€å‚è€ƒã«ãªã‚‹ã‚ˆã†ãªã€ãªã‚‰ãªã„ã‚ˆã†ãªãƒ¢ãƒ‰ã‚«ã‚·ã‚µğŸ˜…"},"5":{"comment":"ç§ãŒçŸ¥ã‚ŠãŸã„ã“ã¨ã¯ä¸€ã¤ã‚‚è¦‹ã„ã ã›ãªã‹ã£ãŸã®ã§ã™ã€‚ã“ã‚ŒãŒé€šè²©ã®ã‚€ã¤ã‹ã—ã„ã¨ã“ã‚ã§ã™ã­ã€‚"},"6":{"comment":"åŸºæœ¬ã‹ã‚‰è§£ã‚Šã‚„ã™ãé«˜é½¢è€…ã«ã¯ãƒ”ãƒƒã‚¿ãƒªã¨æ„Ÿã˜ã¾ã—ãŸã€‚"},"7":{"comment":"ä½¿ã„ã“ãªã›ã‚‹æ§˜ã«ãªã£ãŸã€‚"},"8":{"comment":"çŸ¥ã‚ŠãŸã„ã“ã¨ãŒç†è§£ã§ããªã„å ´åˆãŒå¤šã„"},"9":{"comment":"ä½¿ã„æ–¹ãŒã‚ã‹ã‚‰ãªã„æ™‚ã«ä½¿ã£ã¦ã„ã¾ã™ã€‚ç´¢å¼•ãŒç„¡ã„ã®ã§ä½¿ã„ã«ãã„ã€‚"},"10":{"comment":"ã‚ˆã‹ã£ãŸã§ã™"},"11":{"comment":"æ˜“ã—ã„æ–‡ç« ã¨å†™çœŸã§å¾©ç¿’ã«æœ€é©ã§ã™ã€‚"},"12":{"comment":"ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã‚¢ã‚¤ãƒ›ãƒ³ã®åŒæ–¹ã‚’èª¬æ˜ã—ã¦ã„ã‚‹ã®ãŒè‰¯ã„ã€‚"},"13":{"comment":"iPhoneä½¿ç”¨è€…ã«å‚¾ã„ãŸå†…å®¹ã§ã€Androidä½¿ç”¨è€…ã®ç§ã«ã¯ä¸æº€è¶³ãªã¨ã“ã‚ãŒã‚ã£ãŸã€‚"},"14":{"comment":"ã‚‚ã£ã¨é©šãã‚ˆã†ãªä½¿ã„æ–¹ãŒæœ‰ã‚‹ã‹ã¨æœŸå¾…ã—ãŸãŒã€ã‚ã¾ã‚Šç§ã«ã¯å‚è€ƒã«ã¯ãªã‚‰ãªã‹ã£ãŸã€‚"},"15":{"comment":"å†…å®¹ã¯ä¸€å¿œãƒ¡ãƒ¼ãƒ«ã¨Webç¨‹åº¦ã¯å‡ºæ¥ã‚‹äººå‘ã‘ã€‚ã¾ã£ãŸãã®åˆå¿ƒè€…ã«ã¯çŒ«ã«å°åˆ¤ã€‚"},"16":{"comment":"å®¶ã®è¦ªã«å†…å®¹ã‚’æ•™ãˆã¦ã‚ã’ã‚‰ã‚Œã‚‹ã€è‰¯ã„æœ¬ã§ã—ãŸã€‚å‚è€ƒã«ãªã‚Šã¾ã—ãŸ"},"17":{"comment":"ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã‚¢ã‚¤ãƒ•ã‚©ãƒ³åˆ¥ã«è§£èª¬ã¯ã—ã¦ã‚ã‚‹ãŒæ©Ÿç¨®ã«ã‚ˆã£ã¦ã‹ãªã‚Šç”»é¢ãŒé•ã£ã¦æ¥ã‚‹ã®ã§ãŠå¹´å¯„ã‚Šã«ã¯ç†è§£å›°é›£ã€‚ã‚„ã¯ã‚Šè§¦ã‚ŠãªãŒã‚‰é–“é•ãˆãªãŒã‚‰è¦šãˆã¦è¡Œãã®ãŒé ­ã«å…¥ã‚‹ã€‚é–“é•ãˆãŸã‚‰æˆ»ã‚Šæ–¹ã‚’è§£èª¬ã™ã‚‹ã¨ã‹â€¦ã€‚"},"18":{"comment":"å¾ŒæœŸé«˜é½¢è€…ã®ä¸»äººãŒè³¼å…¥ç°¡ç´ ã«èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã®ã§èª­ã¿ã‚„ã™ã„é …ç›®æ¯ã«æ¢ã—å®‰ãå›°ã‚Šäº‹ãŒåˆ†ã‹ã‚Šæ˜“ã„"},"19":{"comment":"å„ªã—ãã¦åˆ†ã‹ã‚Šæ˜“ã„ã®ã§åŠ©ã‹ã‚Šã¾ã—ãŸã€‚"},"20":{"comment":"åŠåˆ†ã»ã©ã¯ã™ã§ã«çŸ¥ã£ã¦ã„ã‚‹ã“ã¨ã°ã‹ã‚Šã§ã—ãŸãŒã€åˆã‚ã¦çŸ¥ã£ãŸã“ã¨ã‚‚ã‚ã£ã¦ã¾ã‚ã€å½¹ã«ç«‹ã¡ã¾ã—ãŸã€‚"},"21":{"comment":"ã‚¿ã‚¤ãƒˆãƒ«ç¨‹æ˜“ã—ãã¯ãªã„ã€‚æœªè§£æ±ºãŒãŸãã•ã‚“æ®‹ã‚‹ã€‚"},"22":{"comment":"æ–‡å­—ã‚‚å¤§ããã€èª­ã¿æ‰‹å´ã«ç«‹ã£ã¦ã„ã¦ã€è¦ªåˆ‡ä¸å¯§ã€ã‚ã‹ã‚Šã‚„ã™ã‹ã£ãŸã§ã™ã€‚"},"23":{"comment":"ä»Šã®70æ­³ä»£ã¯ã¾ã ã¾ã å‡ºæ¥ã¾ã™ã€‚ã“ã‚“ãªè–„ã£ãºã‚‰ã§ã€å†…å®¹ã®ãªã„æœ¬ãŒ1,650å††ï¼ˆç¨è¾¼ã¿ï¼‰ã¨ã¯é©šãã§ã™ã€‚ã¾ã‚‹ã§ã€Œé«˜é½¢è€…ã‚’é¤Œã«ã—ã¦é‡£ã‚Šã‚ã’ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã€ã¯è¨€ã„éãã§ã—ã‚‡ã†ã‹ã€‚èŒ¶é£²ã¿è©±ãŒåŠåˆ†ã§ã€ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰ã¨ã‚¢ã‚¤ãƒ•ã‚©ãƒ³ç‰ˆãŒã‚ã‚Šå®Ÿè³ªï¼‘/4ã®é‡ã§ã™ã€‚ãã‚‚ãã‚‚80ï¼Œ90æ­³ä»£ã«ãªã‚Œã°ã“ã†ã„ã£ãŸæœ¬ã‚’èª­ã‚€ã®ã‹ã©ã†ã‹åˆ†ã‹ã‚Šã¾ã›ã‚“ãŒã€‚"},"24":{"comment":"åˆ†ã‹ã‚Šã‚„ã™ã„"},"25":{"comment":"è¡¨é¡Œã‹ã‚‰ã™ã‚‹ã¨æœŸå¾…ã§ãã‚‹å†…å®¹ã§ã™ãŒå°‘ã—ã‚¹ãƒãƒ›ä½¿ã„æ…£ã‚ŒãŸäººã«ã¨ã£ã¦ã¯å½“ãŸã‚Šå‰ã®å†…å®¹ã§ã—ãŸã€‚androidã‚¹ãƒãƒ›ã§ã¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹ã¨æ›¸ã„ã¦ã‚ã‚‹é€šã‚Šã®ç”»é¢å§”ãªã‚‰ãªã„ã€ãƒ¡ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã£ã¦ã¯å…¨ãè©²å½“é …ç›®ãŒéš ã‚Œã¦ãŠã‚Šè¦‹ã¤ã‹ã‚‰ãªã„ã€æ¥µç«¯ã®ä¾‹ã§ã¯Kå›½ã®æ©Ÿç¨®ã§ã¯ QRã‚³ãƒ¼ãƒ‰ãŒèª­ã‚ãšã€è§£æ±ºæ–¹æ³•ã¯LINEã®æ©Ÿèƒ½ã§èª­ã¿å–ã‚‹å¿…è¦ãŒã‚ã‚‹ãªã©ãƒãƒƒãƒˆã«ã‚‚ã„ã‚ã„ã‚ã¨è§£æ±ºæ–¹æ³•ãŒç™ºè¡¨ã•ã‚Œã¦ã„ã‚‹ã€‚ä½™åˆ†ãªæ–‡ç« ãŒå¤šãã€androidæ©Ÿç¨®ã«å¯¾ã™ã‚‹è¨˜äº‹ã®å†…å®¹ãŒæ¯”è¼ƒæ¤œè¨ãŒä¸ååˆ†ã§ã‚ã‚‹ã€‚å°‘ã—æ…£ã‚ŒãŸäººãªã‚‰ãƒãƒƒãƒˆã§æ¢ã™ãªã©è£œãˆã‚‹ãŒã€åˆå¿ƒè€…å‘ã‘ã§ã¯ãªã„æœ¬ã§ã‚ã‚‹ã€‚"},"26":{"comment":"é«˜é½¢è€…ãŒã‚¹ãƒãƒ›ã‚’ä½¿ã„ã“ãªã›ãŸã‚‰ã€ã¨ã¦ã‚‚ä¾¿åˆ©ã§é ­ã®é‹å‹•ã«ãªã‚‹ã¨æ€ã„ã¾ã™ãŒã€æ®‹å¿µãªãŒã‚‰ç§ã«ã¯æ›¸ã„ã¦ã‚ã‚‹ã“ã¨ã®ãŠãŠã‚ˆãã¯ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã„ã¾ã—ãŸã®ã§å°‘ã—ç‰©è¶³ã‚Šãªã‹ã£ãŸã‹ï¼Ÿã¨ã„ã†ã¨ã“ã‚ã§ã€æ˜Ÿ3ã¤ã§ã™åŠ›ãŒã¤ã‘ã°æ¨æ¸¬ã§ããŸã‚Šã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã‹ã‚‰ã€ãã®ç‚¹ã§ã¯æ°—æ¥½ã«èª­ã‚ã‚‹æœ¬ã ã¨æ€ã„ã¾ã™"},"27":{"comment":"åˆ†ã‹ã‚Šã«ãã„ã€‚"},"28":{"comment":"80æ­³éããŸçŸ¥ã‚Šåˆã„ã®ç‚ºè³¼å…¥ã—ã¾ã—ãŸä¸­ã€…ã‚¹ãƒãƒ›ã®æ“ä½œã¯é›£ã—ã„ã§ã™ã­"},"29":{"comment":"æ³¨æ–‡ã™ã‚‹å‰ã‹ã‚‰ä¸å®‰ã§ã—ãŸãŒã€ã‚„ã¯ã‚Šç§ã®æ©Ÿç¨®ã§ã¯ä¸æ˜ãªéƒ¨åˆ†ãŒæœ‰ã‚Šã¾ã—ãŸã€‚"},"30":{"comment":"ãªã‚“ã¨ãªããŒç¢ºå®Ÿã«ãªã‚‹ã€ã—ã‹ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„ï¼"},"31":{"comment":"ç§ã®ã‚ˆã†ãªã‚¹ãƒãƒ›åˆå¿ƒè€…ã«ã¨ã£ã¦ã¯ã€èª¬æ˜ãŒæ¯”è¼ƒçš„ã‚ã‹ã‚Šã‚„ã™ããªã£ã¦ã„ã¦å¤§å¤‰å‹‰å¼·ã«ãªã‚Šã¾ã™ã€‚ãªãŠã€æ˜¨ä»Šã¯ç‰¹æ®Šè©æ¬ºã‚„æŠ•è³‡è©æ¬ºã¨ã„ã£ãŸå·§å¦™åŒ–ã—ãŸäº‹è±¡ãŒå ±é“ã•ã‚Œã¦ã€æ³¨æ„ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„äº‹é …ãªã©ã‚‚ã‚ã‚Šã€è¢«å®³è€…ã¨ãªã‚‰ãªã„ã‚ˆã†ã«åŠªã‚ã‚‹æ‰€å­˜ã§ã™ã€‚ã“ã‚Œã‹ã‚‰ã‚‚æ™‚æŠ˜èª­ã¿ãªãŒã‚‰ã€å°‘ã—ãšã¤è¦šãˆã¦ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã„ããŸã„ã¨è€ƒãˆã¦ã„ã‚‹ã€‚"},"32":{"comment":"ä»Šèª­ã‚“ã§ã„ã‚‹ãŒè§£å€Ÿã‚Šã‚„ã™ã„ã§ã™"},"33":{"comment":"ç§ã¯ã€ï¼—ï¼˜æ­³ã«ãªã‚Šã¾ã™ãŒã€åˆã‚ã¦æœ¬æ›¸ã«ã‚ˆã‚Šä¸–ç•Œä¸€ç°¡å˜ãªã‚¹ãƒãƒ›ã®æ“ä½œã¨è¨€ã†äº‹ã§ä¸€ã¤ãšã¤æœ¬ã‚’è¦‹ãªãŒã‚‰ã€æ“ä½œã®ä»•æ–¹ã‚’ç¢ºèªã—ã¦ã„ã¾ã™ã€‚åªä»Šã€ï¼‘ï¼’ï¼é ã§ï¼—ï¼æ­³ã‹ã‚‰ã®ã‚¹ãƒãƒ›ã€Œã¤ã¾ã¥ãåŸå› ãƒ™ã‚¹ãƒˆï¼“ã€ã€ã“ã®æœ¬ã«å¾“ã£ã¦ä¸€ã¤ä¸€ã¤ç¢ºèªã—ã¦ã„ã‘ã°ã‚¯ãƒªã‚¢ï¼å‡ºæ¥ã¾ã™ã€‚å…¨ã¦ã®é“ã¯ã€Œãƒ›ï¼ãƒ ã€ã«é€šã˜ã‚‹ã§ãƒ›ï¼ãƒ ç”»é¢ã«æˆ»ã‚Šæ“ä½œã—ã¦æœ¬æ›¸ã®æ‰‹é †ã«æ·»ã£ã¦ç¢ºèªã—ã¦è¡Œã‘ã°è§£ã£ã¦ãã¾ã™ã€‚ç§ã¯ã¾ã ç´„ï¼•ï¼ï¼…ã‚¯ãƒªã‚¢ï¼ã§ã™ãŒã€æ…Œã¦ãšã€ç„¦ã‚‰ãšã€è«¦ã‚ãªã„ã§æ™‚é–“ã®ã‚ã‚‹æ™‚ã«å°‘ã—ã¥ã¤å‰ã«é€²ã‚“ã§ã„ã¾ã™ã€‚"},"34":{"comment":"çŸ¥ã£ã¦ã¦ã„ã¦ã‚‚ç¢ºèªâ€¦ç­‰ã®ãŸã‚ã«ä¾¿åˆ©ã§ã™"},"35":{"comment":"ä¾¿åˆ©ãªæ©Ÿèƒ½ã‚’ãŸãã•ã‚“æ›¸ã„ã¦ã„ã‚‹æœ¬ã‚’èª­ã‚“ã§ã‚‚ã€ä¼ã‚ã‚‰ãªã‘ã‚Œã°ã€è‡ªå·±å«Œæ‚ªã«è½ã¡ã„ã‚‹ã ã‘ã€‚ãŠã¾ã‘ã«ç”Ÿæ´»ãªå¿…è¦ãªæ©Ÿèƒ½ã¯ã€ãã‚“ãªã«ãŸãã•ã‚“ã¯è¦ã‚‰ãªã„ã€‚ã“ã®æœ¬ã¯æœ¬å½“ã«å¿…è¦ãªæƒ…å ±ã‚’åˆ†ã‹ã‚Šã‚„ã™ãæ•™ãˆã¦ãã‚Œã¾ã™ã€‚æ–‡å­—ã‚‚å¤§ãã„ã®ã§èª­ã¿ã‚„ã™ã„ã€‚"},"36":{"comment":"70æ­³å¥³æ€§ã€‚ã¾ã£ãŸãå½¹ã«ãŸãŸãªã‹ã£ãŸã€‚çŸ¥ã£ã¦ã„ã‚‹äº‹ã—ã‹æ›¸ã„ã¦ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"},"37":{"comment":"ï¼—ï¼ä»£ã®æ¯ã«è´ˆç­”ã—ã¾ã—ãŸã€‚å›³æ›¸é¤¨ã§å€Ÿã‚Šã¦ãã¦ä¸€åº¦æ‹èª­ã¯ã—ãŸã®ã§ã™ãŒæ–‡å­—ãŒå¤§ããèª¬æ˜ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„ã‚‰ã—ãã¦æ‰‹å…ƒã«ç½®ã„ã¦ãŠããŸã„ã¨ã®äº‹ã§ã—ãŸã®ã§è³¼å…¥ã—ã¾ã—ãŸã€‚"},"38":{"comment":"70æ­³ã‚’ã™ããŸç§ã§ã‚‚ç†è§£ã§ãã‚‹å†…å®¹ã§ã—ãŸã€‚"},"39":{"comment":"å†…å®¹ãŒåˆ†ã‹ã‚Šã‚„ã™ãã€ç†è§£ã—ã‚„ã™ã„ã§ã™ã€‚"},"40":{"comment":"ã‚¹ãƒãƒ›ã‚’è‰²ã€…ã«ä½¿ã„ã¾ãã‚‹æœ¬ã§ã€ãŠãŠã„ã«å‚è€ƒã«ãªã‚Šã¾ã—ãŸã€‚"},"41":{"comment":"40æ­³ã§ã‚‚å……åˆ†éãã‚‹è³‡æ–™æ€§é«˜ã„ï¼"},"42":{"comment":"70ä»£ã§ã‚¹ãƒãƒ›ãƒ‡ãƒ“ãƒ¥ãƒ¼ã—ãŸçˆ¶è¦ªã«è³¼å…¥ã—ã¾ã—ãŸã€‚ä¸€ç·’ã«æš®ã‚‰ã—ã¦ã„ãªã„ã®ã§ã€ãªã‹ãªã‹ä½¿ã„æ–¹ã‚’æ•™ãˆã¦ã‚ã’ã‚‰ã‚Œãªã„ã®ã§ã“ã¡ã‚‰ã®æœ¬ã‚’é€ã‚Šã¾ã—ãŸã€‚å…ƒã€…ãƒ‘ã‚½ã‚³ãƒ³ãªã©ã¯å¥½ããªã®ã§ç‹¬åŠ›ã§ãƒã‚¹ã‚¿ãƒ¼ã—ã¦ã‚‚ã‚‰ãŠã†ã¨æ€ã„ã¾ã™ã€‚"},"43":{"comment":"ãŸã ã»ã¨ã‚“ã©ã®äº‹ä¾‹ã¯ã™ã§ã«çŸ¥ã£ã¦ã„ãŸã®ã§ã€ã¡ã‚‡ã£ã¨æ—©åˆç‚¹ã§è²·ã£ã¦ã—ã¾ã£ãŸã‹ãªã€ã¨æ€ã„ã¾ã—ãŸã€‚"},"44":{"comment":"å°é›£ã—ã„äº‹ã¯å…¨ç„¶ãªã„ã—ã€åˆæ­©ã®åˆæ­©çŸ¥è­˜ãŒã€å¾—ã‚‰ã‚Œæº€è¶³ã§ã™ã€‚å¹´é½¢ãŒä¸ŠãŒã‚‹ã¨ä½•ãŒåˆ¤ã‚‰ãªã„ã‹ãŒåˆ¤ã‚‰ãªã„ã®ã§ã“ã‚Œãã‚‰ã„ã§ã¡ã‚‡ã†ã©è‰¯ã„ã€‚ãã‚Œã‹ã‚‰å…ˆã¯å„ã€…ã§çŸ¥è­˜ã‚’å¾—ã‚Œã°è‰¯ã„ã¨æ€ã„ã¾ã™ã€‚"},"45":{"comment":"è‰²ã€…ãªæ“ä½œã®ã‚„ã‚Šæ–¹ã‚’çŸ¥ã‚ŠãŸã‹ã£ãŸãŒã€æ­³ã‚’ã¨ã£ãŸè©±ã—ã¨ã‹ãŒå¤šãæœŸå¾…å¤–ã‚Œã§æ®‹å¿µã€‚"},"46":{"comment":"å¤šåˆ†ã€ä¸‡äººã«ã¡ã‚‡ã†ã©ã„ã„ã€ã¨ã„ã†ã®ã¯é›£ã—ã„ã®ã ã¨æ€ã„ã¾ã™ãŒã€ã‚‚ã†ã¡ã‚‡ã£ã¨å†…å®¹ãŒã‚ã£ã¦ã‚‚è‰¯ã‹ã£ãŸã‹ãªï¼Ÿã‚ˆãä½¿ã„æ–¹ã«ã¤ã„ã¦ä¸¡è¦ªãŒèã„ã¦ãã‚‹ã®ã§ã€å‚è€ƒã«ãªã‚Œã°ã€ã¨è³¼å…¥ã€‚å­—ãŒå¤§ããã€ã‚¹ãƒšãƒ¼ã‚¹ã‚‚åºƒãã¦èª­ã¿ã‚„ã™ã•ã¯ã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚ãŸã ã€å†…å®¹çš„ã«ã¯ã‚‚ã†ã²ã¨è¶…ãˆã‚ã£ã¦ã‚‚è‰¯ã„ã‹ã‚‚ã€‚ã‚¢ãƒ—ãƒªã®ã¾ã¨ã‚æ–¹ã¯æ›¸ã„ã¦ã‚ã£ã¦ã‚‚ã€ä¸¦ã¹æ›¿ãˆã®ä»•æ–¹ã¯æ›¸ã„ã¦ãªã„ã€ãªã©ã€ã¡ã‚‡ã£ã¨æ®‹å¿µã€‚ã‚ã‚‹ç¨‹åº¦ä½¿ãˆã‚‹äººãªã‚‰ã€ç‰©è¶³ã‚Šãªã•ã¯ã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚ç¶šç·¨ã€ä¸Šç´šç·¨ãªã©ã‚ã£ãŸã‚‰è‰¯ã„ã‹ã—ã‚‰ï¼Ÿ"},"47":{"comment":"ã‚¹ãƒãƒ›ã®æ“ä½œãŒåˆ†ã‹ã‚‰ãªã„ã¨æ‚©ã‚€æ¯ã¸ã€æœ¬ã®é¡Œåã«ã¤ã‚‰ã‚Œè³¼å…¥ã—ã¾ã—ãŸã€‚æœ¬ã®è³¼å…¥æ™‚ã€ä¸­èº«ã‚’è©¦ã—è¦‹ãŒå‡ºæ¥ãªã‹ã£ãŸã®ã§ã€é¡Œåã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å‚è€ƒã«è³¼å…¥ã—ã¾ã—ãŸã€‚åŸºæœ¬çš„ãªã‚¹ãƒãƒ›ã®æ“ä½œæ–¹æ³•ãŒè¼‰ã£ã¦ã„ã‚‹ã®ã‹ã¨æ€ã£ãŸã‚‰ã€ã“ã†ã„ã†å ´åˆã¯ã“ã†ã„ã†å¯¾å‡¦ã‚’ã™ã‚‹ã¨è¨€ã†å†…å®¹ã®ç‰©ãŒä¸»ã§ã™ã€‚ã‚ã‚‹ç¨‹åº¦ã‚¹ãƒãƒ›ã®çŸ¥è­˜ãŒã‚ã‚‹æ–¹ãªã‚‰ç†è§£å‡ºæ¥ã‚‹å†…å®¹ã ã¨æ€ã„ã¾ã™ãŒã€åˆå¿ƒè€…ãŒã“ã¡ã‚‰ã®æœ¬ã‚’è¦‹ã¦ç†è§£ã™ã‚‹ã«ã¯é›£ã—ã„ã¨æ€ã„ã¾ã™ã€‚"},"48":{"comment":"å¤§ä½“ã€çŸ¥ã£ã¦ã„ã‚‹äº‹ã°ã‹ã‚Šã§ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ã‚¹ã®äº‹ã‚’çŸ¥ã‚ŠãŸã‹ã£ãŸã®ã§ã™ãŒã€PayPayã ã‘ã§ã™ã‹?ä»–ã«ã¯ç„¡ã„ã®ã§ã—ã‚‡ã†ã‹?"},"49":{"comment":"ã“ã®ã‚ˆã†ãªã“ã¨ãŒåˆ†ã‹ã‚‰ãªã„70æ­³ä»¥ä¸Šã®äººãŒã©ã‚Œã»ã©ã„ã‚‹ã®ã‹ãª? è¿”å“ã—ãŸã„æ°—æŒã¡!"},"50":{"comment":"LINEã®ç™»éŒ²ã¯ç›¸æ‰‹ã‹ã‚‰ã®èªè¨¼ãŒå¿…è¦ï¼Ÿè¿·æƒ‘ãƒ¡ãƒ¼ãƒ«ã®è¨­å®šã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã‹ã£ãŸã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒè§£ã‚‰ãªã„æ™‚ãƒ‘ã‚¹ã‚³-ãƒ‰ãŒå±Šã‹ãªã„ã€‚"},"51":{"comment":"çŸ¥ã‚Šåˆã„ã®äººã«ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã—ãŸã¨ã“ã‚ã€æ¬¡ã«ä¼šã£ãŸæ™‚ã«ã™ã”ãå–œã‚“ã§è‡ªæ…¢ã‚’ã—ã¦ãã¾ã—ãŸç¬‘ã‚¹ãƒãƒ›ãŒã†ã¾ãä½¿ãˆãªã„äººã«ã¨ã£ã¦ãƒ»ã‚¹ãƒãƒ›ã®ç´°ã‹ã„æ©Ÿèƒ½ã€ä»•çµ„ã¿ãªã‚“ã‹ã¯ã©ã†ã§ã‚‚ã‚ˆãã¦ã€ãƒ»å›°ã£ã¦ã„ã‚‹ã“ã¨ãŒè§£æ¶ˆã—ãŸã‚Šãƒ»è‡ªåˆ†ã«ã‚ã£ãŸæ–°ã—ã„ä½¿ã„æ–¹ãŒè¦‹ã¤ã‹ã£ãŸã‚Šãƒ»å­ä¾›ã«èã„ã¦ã€Œã“ã‚“ãªã“ã¨ã‚‚åˆ†ã‹ã‚‰ãªã„ã®ï¼Ÿã€ã¨è¨€ã‚ã‚Œãªããªã‚‹ã¿ãŸã„ãªã“ã¨ã®æ–¹ãŒå–œã°ã‚Œã‚‹ã‚“ã ãªã€ã¨æ„Ÿã˜ã¾ã—ãŸç¬‘ã“ã‚Œã¯ã€ŒæŠ€è¡“æ›¸ã€ã˜ã‚ƒãªãã¦ã€Œç”Ÿæ´»ã«ã€ã†ã¾ãä½¿ãˆã‚‹å°æŠ€é›†ã€ã¿ãŸã„ãªæ„Ÿã˜ã§ã™ã‹ã­ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã—ãŸã‚‰å–œã‚“ã§ã‚‚ã‚‰ãˆã¦å¬‰ã—ã‹ã£ãŸã®ã§â˜…5ã§ã™ï¼"},"52":{"comment":"æœ¬æ›¸ã¯ã‚¹ãƒãƒ›æ´»ç”¨ã®ãƒã‚¦ãƒ„ãƒ¼æ›¸ã«ç•™ã¾ã‚‰ãšã€é«˜é½¢è€…ãŒã‚¹ãƒãƒ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚·ãƒ‹ã‚¢ãƒ©ã‚¤ãƒ•ã«æ´»åŠ›ã¨æ¥½ã—ã¿ã‚’äº«å—ã§ãã‚‹ç´ æ™´ã‚‰ã—ã•ã‚’ä¼ãˆãŸã„ã€‚ãã‚“ãªè‘—è€…ã‹ã‚‰ã®ã‚·ãƒ‹ã‚¢ã«å¯„ã‚Šæ·»ã†æ¸©ã‹ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ„Ÿã˜ã‚‹è‡ªå·±å•“ç™ºæ›¸ã¨ã‚‚è¨€ãˆã‚‹ã€‚å†…å®¹ã¯ãƒ»ã‚¹ãƒãƒ›ã‚’ç”Ÿæ´»ã®ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã™ã‚‹ãƒã‚¤ãƒ³ãƒ‰ãƒ»ãµã‚“ã ã‚“ãªæ´»ç”¨äº‹ä¾‹ãƒ»ã‚·ãƒ‹ã‚¢ã®ä¸å®‰è¦ç´ ã®è§£æ±ºãƒ»é …ç›®æ¯ã®å›³è§£ã«ã‚ˆã‚‹å„ç¨®è¨­å®šã‚„æ“ä½œæ–¹æ³•ã“ã‚Œã‚‰ã‚’èª­ã¿ã‚„ã™ã„å¤§ããªæ–‡å­—ã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§ç·¨é›†ã•ã‚Œã€ãƒ‡ã‚¸ã‚¿ãƒ«æ™‚ä»£ã®å°‚é–€ç”¨èªã¯ã‚·ãƒ‹ã‚¢ãŒçµŒé¨“ã—ã¦ããŸæ™‚ä»£èƒŒæ™¯ã‚’è€ƒæ…®ã—ãŸè¨€è‘‰ã«ç½®ãæ›ãˆã¦æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚20å¹´ä»¥ä¸Šè‡ªèº«ã®ãƒ‘ã‚½ã‚³ãƒ³æ•™å®¤ã§ã‚·ãƒ‹ã‚¢ã«å‘ãåˆã£ã¦ããŸã€è‘—è€…ãªã‚‰ã§ã¯ã®ç§€é€¸ã•ã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã§ããŸã‚·ãƒªãƒ¼ã‚ºæœ€æ–°æœ¬ã§ã™ã€‚"},"53":{"comment":"ã‚¹ãƒãƒ›ã®ä½¿ã„æ–¹ã ã‘ã§ãªãã‚·ãƒ‹ã‚¢ã ã‹ã‚‰ã“ãã®ä½¿ã„æ–¹ã€ã‚¹ãƒãƒ›ã¨ã®å‘ãåˆã„æ–¹ãŒã‚ã‹ã‚Šã¾ã™ã€‚æ™®æ®µã‹ã‚‰ã‚·ãƒ‹ã‚¢ä¸–ä»£ã¨æ¥ã—ã¦ã„ã‚‹è‘—è€…å¢—ç”°å…ˆç”Ÿã ã‹ã‚‰ã“ãâ€¦ã®å†…å®¹ã§ã™ã€‚ã‚¹ãƒãƒ›ã§ã„ã‚ã„ã‚ã§ãã‚‹ã¨è¨€ã‚ã‚Œã¦ã‚‚å…·ä½“çš„ã«ä½•ã«ã©ã†ä½¿ã£ãŸã‚‰ã‚ˆã„ã®ã‹ã‚ã‹ã‚‰ãªã„ã€äººã«ã‚‚èã‘ãªã„ã€èãæ–¹ãŒã‚ã‹ã‚‰ãªã„ã¨ã„ã†æ–¹ã¯ä¸€é€šã‚Šèª­ã‚“ã§è©¦ã—ã¦ã¿ã‚‹ã®ã‚‚ã‚ˆã„ã§ã—ã‚‡ã†ã€‚ã‚·ãƒ‹ã‚¢ä¸–ä»£ã®ãƒªã‚¢ãƒ«ãªä½“é¨“è«‡ã‚‚è¼‰ã£ã¦ã„ã¾ã™ã€Œãã†ãã†ã“ã†ã„ã†æ™‚å›°ã‚‹ï¼ã€ã€Œã¸ã‡ã“ã†ã„ã†é¢¨ã«ä½¿ã£ã¦ã„ã‚‹äººã‚‚ã„ã‚‹ã‚“ã ã€ã¨å…±æ„Ÿã—ãŸã‚Šã€æ–°ã—ã„ç™ºè¦‹ã‚’ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã§ã—ã‚‡ã†ã€‚"},"54":{"comment":"è¡¨ç´™ã®ã‚¿ã‚¤ãƒˆãƒ«é€šã‚Šã€äººã«èã‹ãªãã¦ã‚‚ã€è§£æ±ºã—ãŸã„å›°ã‚Šäº‹ã‚ã‚‹ã‚ã‚‹ãŒè¼‰ã£ã¦ã¾ã™ï¼è¡Œé–“éš”ãŒåºƒã„ã®ã§èª­ã¿ã‚„ã™ã„ã—ã€ã‚¤ãƒ©ã‚¹ãƒˆã§ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ã‚ã‚Šã¾ã™ã‚ˆã€‚ãƒ‘ãƒ©ãƒ‘ãƒ©ã‚ãã£ã¦ã€èª­ã¿ãŸã„ã¨ã“ã‚ãŒãƒ‘ãƒƒã¨æ¢ã—ã‚„ã™ã„ã§ã™ã€‚å›°ã‚Šäº‹ãŒã€ã‚ã‹ã‚‹æ¥½ã—ã•ã«å¤‰ã‚ã‚‹æœ¬ã§ã™ã€‚"}},"translations":{},"overview":"å…¬çš„ãªæ„è¦‹åé›†ã®çµæœã€é«˜é½¢è€…å‘ã‘ã®æƒ…å ±æä¾›ã«ã¯ç°¡æ½”ã§ã‚ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ãŒæ±‚ã‚ã‚‰ã‚Œã€å®Ÿè·µçš„ãªå­¦ç¿’ãŒåŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ãŒå¼·èª¿ã•ã‚Œã¾ã—ãŸã€‚æ›¸ç±ã®å†…å®¹ã«ã¤ã„ã¦ã¯ã€è¦–è¦šçš„è£œåŠ©ãŒè©•ä¾¡ã•ã‚Œã‚‹ä¸€æ–¹ã§ã€ç´¢å¼•ã®æ¬ å¦‚ã‚„æ·±ã•ã®ä¸è¶³ãŒæŒ‡æ‘˜ã•ã‚Œã€ç‰¹ã«æ—¢å­˜ã®çŸ¥è­˜ã‚’æŒã¤èª­è€…ã«ã¯ç‰©è¶³ã‚Šãªã•ãŒæ„Ÿã˜ã‚‰ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€ã‚¹ãƒãƒ›æ“ä½œã«é–¢ã—ã¦ã¯iPhoneã«åã£ãŸæƒ…å ±ãŒå¤šãã€Androidãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®å†…å®¹ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨ã®æ„è¦‹ãŒã‚ã‚Šã€ã‚ˆã‚Šå¤šæ§˜ãªæ©Ÿç¨®ã¸ã®å¯¾å¿œãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚","config":{"name":"ä¸–ç•Œä¸€ç°¡å˜ï¼ 70æ­³ã‹ã‚‰ã®ã‚¹ãƒãƒ›ã®ä½¿ã„ã“ãªã—è¡“","question":"ã€ä¸–ç•Œä¸€ç°¡å˜ï¼ 70æ­³ã‹ã‚‰ã®ã‚¹ãƒãƒ›ã®ä½¿ã„ã“ãªã—è¡“ã€ã«ã¤ã„ã¦ã€èª­è€…ã‹ã‚‰ã®ä¸»ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ä½•ã§ã™ã‹ï¼Ÿ","input":"review","model":"gpt-4o-mini","extraction":{"workers":3,"limit":100,"prompt_file":"review","source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆå†…ã‹ã‚‰ä¸»è¦ãªè­°è«–ã‚’æŠ½å‡ºã™ã‚‹è­°è«–æŠ½å‡ºè€…ã§ã™ã€‚\nå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¯ã€Amazonã®æœ¬ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\nå„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã¯ã€è­°è«–ã‚’å«ã‚€å½¢å¼ã§Pythonã®ãƒªã‚¹ãƒˆã¨ã—ã¦å›ç­”ã—ã¾ã™ã€‚\nå„è­°è«–ã¯ã€ãã‚Œè‡ªä½“ã§ç†è§£å¯èƒ½ã§èª­ã¿ã‚„ã™ã„ã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\nè­°è«–ã¯ã€ä»–ã®è­°è«–ã‚„å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã›ãšã€ä¸€äººç§°ã§è¡¨ç¾ã—ã€ä¸€èˆ¬çš„ãªçœŸå®Ÿã‚’ç¾åœ¨å½¢ã§è¿°ã¹ã¾ã™ã€‚","model":"gpt-4o-mini"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"intro":"2025å¹´2æœˆ9æ—¥æ™‚ç‚¹ã§åé›†ã•ã‚ŒãŸ54ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«åŸºã¥ãã€é¡§å®¢ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®åˆ†æçµæœã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚","output_dir":"review","previous":{"name":"Smartphone Usage Guide for Seniors","question":"What are the main points of feedback from readers about the book 'ä¸–ç•Œä¸€ç°¡å˜ï¼ 70æ­³ã‹ã‚‰ã®ã‚¹ãƒãƒ›ã®ä½¿ã„ã“ãªã—è¡“'?","input":"review","model":"gpt-4o-mini","extraction":{"workers":3,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"translation":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."},"intro":"The following is an analysis of customer reviews for the book 'ä¸–ç•Œä¸€ç°¡å˜ï¼ 70æ­³ã‹ã‚‰ã®ã‚¹ãƒãƒ›ã®ä½¿ã„ã“ãªã—è¡“'. This summary is based on 54 reviews collected as of 2025-02-09.","output_dir":"review","embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"completed","start_time":"2025-02-09T13:09:45.469843","completed_jobs":[{"step":"extraction","completed":"2025-02-09T13:10:17.168788","duration":31.69538,"params":{"workers":3,"limit":50,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nYou are a professional research assistant and your job is to help \nme prepare a nice and clean datasets of arguments. \n\nThe context is that we have run a public consultation on the \ntopic of artificial intelligence. I'm going to give you examples \nof arguments that were contributed by the public and I want you \nto help me make them more concise and easy to read. When really \nnecessary, you can also break it down into two separate arguments, \nbut it will often be best to return a single arguments. \n\nPlease return the result as a well-formatted JSON list of strings. \n\n/human\n\nAI technologies should be developed with a focus on reducing their own \nenvironmental impact over their lifecycle.\n\n/ai \n\n[\n  \"We should focus on reducing the environmental impact of AI technologies\"\n]\n\n/human \n\nThere should be a concerted effort to educate the public about the \ncapabilities, limitations, and ethical considerations of AI.\n\n/ai \n\n[\n  \"We should educate the public about the capabilities of AI\",\n  \"We should educate the public about the limitations and ethical considerations of AI\"\n]\n\n/human \n\nAI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\n\n/ ai \n\n[\n  \"AI can optimize smart homes and buildings for energy efficiency and occupant wellbeing.\"\n]\n\n/human \n\nAI can help optimize energy grids, reducing waste and carbon emissions.\n\n/ai \n\n[\n  \"AI could optimize energy grids to reduce waste and carbon emissions.\"\n]\n\n","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-02-09T13:10:21.788692","duration":4.618299,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2025-02-09T13:10:31.708968","duration":9.919167,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2025-02-09T13:10:33.441345","duration":1.73174,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-02-09T13:10:43.065151","duration":9.621606,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-02-09T13:10:45.727661","duration":2.658937,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"}},{"step":"translation","completed":"2025-02-09T13:11:52.793903","duration":67.062177,"params":{"model":"gpt-4o-mini","languages":["Japanese"],"flags":["JP"],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list."}},{"step":"aggregation","completed":"2025-02-09T13:11:52.830985","duration":0.034075,"params":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"}},{"step":"visualization","completed":"2025-02-09T13:12:00.454150","duration":7.621994,"params":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"}}],"lock_until":"2025-02-09T13:17:00.455570","current_job":"visualization","current_job_started":"2025-02-09T13:11:52.832213","translation_prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.","previously_completed_jobs":[],"end_time":"2025-02-09T13:12:00.455564"},"embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"},"translation":{"languages":[],"flags":[],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"some parameters changed: limit, prompt"},{"step":"embedding","run":true,"reason":"some dependent steps will re-run: extraction"},{"step":"clustering","run":true,"reason":"some dependent steps will re-run: embedding"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"translation","run":true,"reason":"some dependent steps will re-run: extraction, labelling, takeaways, overview"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: extraction, clustering, labelling, takeaways, overview, translation"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"running","start_time":"2025-02-09T15:49:15.440124","completed_jobs":[{"step":"extraction","completed":"2025-02-09T15:49:55.533485","duration":40.089784,"params":{"workers":3,"limit":100,"prompt_file":"review","source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\nã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆå†…ã‹ã‚‰ä¸»è¦ãªè­°è«–ã‚’æŠ½å‡ºã™ã‚‹è­°è«–æŠ½å‡ºè€…ã§ã™ã€‚\nå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¯ã€Amazonã®æœ¬ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\nå„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã¯ã€è­°è«–ã‚’å«ã‚€å½¢å¼ã§Pythonã®ãƒªã‚¹ãƒˆã¨ã—ã¦å›ç­”ã—ã¾ã™ã€‚\nå„è­°è«–ã¯ã€ãã‚Œè‡ªä½“ã§ç†è§£å¯èƒ½ã§èª­ã¿ã‚„ã™ã„ã‚‚ã®ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\nè­°è«–ã¯ã€ä»–ã®è­°è«–ã‚„å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã›ãšã€ä¸€äººç§°ã§è¡¨ç¾ã—ã€ä¸€èˆ¬çš„ãªçœŸå®Ÿã‚’ç¾åœ¨å½¢ã§è¿°ã¹ã¾ã™ã€‚","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-02-09T15:49:57.609041","duration":2.074405,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2025-02-09T15:50:07.793381","duration":10.182643,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2025-02-09T15:50:09.799764","duration":2.004841,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are a category labeling assistant that generates a category label \nfor a set of arguments within a broader consultation. You are given the main question \nof the consultation, list of arguments inside the cluster, and a list of arguments \noutside this cluster. You answer with a single category label that summarizes the \ncluster. \n\nYou do not include context that is already obvious from the question (for example: \nif the question of the consultation is something like \"what challenges are you facing \nin France\", there is no need to repeat \"in France\" in the cluster label).\n\nThe label must be very concise and just precise enough to capture what distinguishes \nthe cluster from the arguments found outside. \n\n/human\n\nQuestion of the consultation: \"What do you think has been the impact of the UK decision to leave the EU?\"\n\nExamples of arguments OUTSIDE the cluster of interest:\n\n * We faced limitations in educational and cultural exchange opportunities due to exclusion from the Erasmus program.\n * The UK dealt with longer travel times caused by increased border checks, affecting commuters and vacationers.\n * We saw reduced cooperation in environmental standards, hindering efforts to combat climate change.\n * I experienced challenges in patient care due to disruptions in reciprocal healthcare agreements.\n * We faced complexity in residency and citizenship applications for families due to Brexit-related changes.\n * The UK witnessed hindrance in global efforts to address research challenges due to reduced collaboration opportunities.\n * We faced limitations in creative projects due to exclusion from EU cultural funding programs.\n * The UK witnessed setbacks in charitable initiatives and community support due to the loss of EU funding.\n * We experienced challenges in cross-border dispute resolution due to weakened consumer protections.\n * The UK faced limitations in touring EU countries as professional musicians, impacting careers.\n\nExamples of arguments inside the cluster:\n\n * We experienced supply chain disruptions due to Brexit, leading to increased costs and delayed deliveries for businesses.\n * I faced market fluctuations and uncertainties in investments and retirement savings because of Brexit.\n * The UK dealt with reduced profit margins as an exporter due to new tariffs and customs procedures.\n * We lost jobs because companies relocated operations to stay within the EU market post-Brexit.\n * The UK struggled with the increased cost of living caused by skyrocketing prices of imported goods.\n * We witnessed a decline in investment in the UK tech sector, impacting innovation and job opportunities.\n * The UK saw a decline in tourism due to new visa regulations, affecting hospitality businesses.\n * I experienced reduced purchasing power and increased travel expenses due to the drop in the pound's value.\n\n/ai \n\nNegative Financial Impact\n","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-02-09T15:50:17.331897","duration":7.530286,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. You will be given a list of arguments that have been made by a cluster of participants during a public consultation. You respond with one or two paragraphs summarizing your main takeaways. You are very concise and write short, snappy sentences which are easy to read. \n \n/human\n\n[\n  \"I firmly believe that gun violence constitutes a severe public health crisis in our society.\",\n  \"We need to address this issue urgently through comprehensive gun control measures.\", \n  \"I support the implementation of universal background checks for all gun buyers\",\n  \"I am in favor of banning assault weapons and high-capacity magazines.\",\n  \"I advocate for stricter regulations to prevent illegal gun trafficking.\",\n  \"Mental health evaluations should be a mandatory part of the gun purchasing process.\"\n]\n\n/ai \n\nParticipants called for comprehensive gun control, emphasizing universal background checks, assault weapon bans, curbing illegal gun trafficking, and prioritizing mental health evaluations.","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-02-09T15:50:20.605403","duration":3.271323,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nYou are an expert research assistant working in a think tank. \nYour team has run a public consultation on a given topic and has \nstarted to analyze what the different cluster of options are. \nYou will now receive the list of clusters with a brief \nanalysis of each cluster. Your job is to return a short summary of what \nthe findings were. Your summary must be very concise (at most one \nparagraph, containing at most four sentences) and you must avoid platitudes. ","model":"gpt-4o-mini"}},{"step":"translation","completed":"2025-02-09T15:50:20.609656","duration":0.002174,"params":{"languages":[],"flags":[],"source_code":"\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [\"Argument\", \"Original comment\", \"Representative arguments\",\n               \"Open full-screen map\", \"Back to report\", \"Hide labels\", \"Show labels\",\n               \"Show filters\", \"Hide filters\", \"Min. votes\", \"Consensus\",\n               \"Showing\", \"arguments\", \"Reset zoom\", \"Click anywhere on the map to close this\",\n               \"Click on the dot for details\",\n               \"agree\", \"disagree\", \"Language\", \"English\", \"arguments\", \"of total\",\n               \"Overview\", \"Cluster analysis\", \"Representative comments\", \"Introduction\",\n               \"Clusters\", \"Appendix\", \"This report was generated using an AI pipeline that consists of the following steps\",\n               \"Step\", \"extraction\", \"show code\", \"hide code\", \"show prompt\", \"hide prompt\", \"embedding\",\n               \"clustering\", \"labelling\", \"takeaways\", \"overview\"]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i < len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) > 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries > 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e\n","prompt":"/system \n\nYou are a professional translator.\nYou will receive a list of words and sentences written in English. \nPlease return the same list, in the same order, but translated to {language}.\nMake sure to return a valid JSON list of string of the same length as the original list.","model":"gpt-4o-mini"}}],"lock_until":"2025-02-09T15:55:20.611493","current_job":"aggregation","current_job_started":"2025-02-09T15:50:20.611487"}}},"__N_SSG":true}